var relearn_searchindex = [
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-prompt-decorator.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nbedrock services: name: service1 host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy instance_name: “ai-proxy-bedrock” config: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: “llm/v1/chat” model: provider: “bedrock” options: bedrock: aws_region: “us-west-2” name: ai-prompt-decorator instance_name: ai-prompt-decorator-bedrock config: prompts: prepend: - role: system content: “You will always respond in the Portuguese (Brazil) language.” EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-prompt-decorator.yaml :::\nSend a request now:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “what is pi?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ | jq :::\nYou can now click Next to proceed further.",
    "description": "The AI Prompt Decorator plugin adds an array of llm/v1/chat messages to either the start or end of an LLM consumer’s chat history. This allows you to pre-engineer complex prompts, or steer (and guard) prompts in such a way that the modification to the consumer’s LLM message is completely transparent.\nYou can use this plugin to pre-set a system prompt, set up specific prompt history, add words and phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Decorator plugin",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/prompt-decorator/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.\nKonnect modules Kong Konnect Enterprise features are described in this section, including modules and plugins that extend and enhance the functionality of the Kong Konnect platform.\nControl Plane (Gateway Manager) Control Plane empowers your teams to securely collaborate and manage their own set of runtimes and services without the risk of impacting other teams and projects. Control Plane instantly provisions hosted Kong Gateway control planes and supports securely attaching Kong Gateway data planes from your cloud or hybrid environments.\nThrough the Control Plane, increase the security of your APIs with out-of-the-box enterprise and community plugins, including OpenID Connect, Open Policy Agent, Mutual TLS, and more.\nDev Portal Streamline developer onboarding with the Dev Portal, which offers a self-service developer experience to discover, register, and consume published services from your Service Hub catalog. This customizable experience can be used to match your own unique branding and highlights the documentation and interactive API specifications of your services. Enable application registration to automatically secure your APIs with a variety of authorization providers.\nAnalytics Use Analytics to gain deep insights into service, route, and application usage and health monitoring data. Keep your finger on the pulse of the health of your API products with custom reports and contextual dashboards. In addition, you can enhance the native monitoring and analytics capabilities with Kong Gateway plugins that enable streaming monitoring metrics to third-party analytics providers.\nTeams To help secure and govern your environment, Konnect provides the ability to manage authorization with teams. You can use Konnect’s predefined teams for a standard set of roles, or create custom teams with any roles you choose. Invite users and add them to these teams to manage user access. You can also map groups from your existing identity provider into Konnect teams.\nFurther Reading Gateway Manager Dev Portal Analytics",
    "description": "The Kong Konnect platform provides a cloud control plane (CP), which manages all service configurations. It propagates those configurations to all Runtime control planes, which use in-memory storage. These nodes can be installed anywhere, on-premise or in AWS.\nFor today workshop, we will be focusing on Kong Gateway. Kong Gateway data plane listen for traffic on the proxy port 443 by default. The data plane evaluates incoming client API requests and routes them to the appropriate backend APIs. While routing requests and providing responses, policies can be applied with plugins as necessary.",
    "tags": [],
    "title": "Kong Konnect Architectural Overview",
    "uri": "/architecture/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.\nThis plugin also sanitizes string inputs to ensure that JSON control characters are escaped, preventing arbitrary prompt injection.\nWhen calling a template, simply replace the messages (llm/v1/chat) or prompt (llm/v1/completions) with a template reference, in the following format: {template://TEMPLATE_NAME}\nHere’s an example of template definition:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-prompt-template.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nbedrock services: name: service1 host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy instance_name: “ai-proxy-bedrock” config: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: “llm/v1/chat” model: name: “us.amazon.nova-lite-v1:0” provider: “bedrock” options: bedrock: aws_region: “us-west-2” name: ai-prompt-template instance_name: ai-prompt-template-bedrock enabled: true config: allow_untemplated_requests: true templates: name: template1 template: |- { “messages”: [ { “role”: “user”, “content”: “Explain to me what {{thing}} is.” } ] } EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-prompt-template.yaml :::\nNow, send a request referring the template:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: “{template://template1}”, “properties”: { “thing”: “niilism” } }’ | jq :::\nKong-gratulations! have now reached the end of this module by authenticating your API requests with AWS Cognito. You can now click Next to proceed with the next module.",
    "description": "The AI Prompt Template plugin lets you provide tuned AI prompts to users. Users only need to fill in the blanks with variable placeholders in the following format: {{variable}}. This lets admins set up templates, which can be then be used by anyone in the organization. It also allows admins to present an LLM as an API in its own right - for example, a bot that can provide software class examples and/or suggestions.",
    "tags": [],
    "title": "AI Prompt Template plugin",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/prompt-template/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced plugin",
    "content": "Let’s get started with a simple round-robin policy:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:\nbedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: balancer: algorithm: round-robin targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy-advanced.yaml :::\nSend the request few times. Note we are going to receive responses from both LLMs, in a round-robin way.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is considered the greatest Polish writer?” } ] }’ | jq :::\nHere’s the Llama3.3 response:\n{ \"model\": \"us.meta.llama3-3-70b-instruct-v1:0\", \"usage\": { \"prompt_tokens\": 43, \"completion_tokens\": 372, \"total_tokens\": 415 }, \"object\": \"chat.completion\", \"choices\": [ { \"message\": { \"content\": \"The question of who is considered the greatest Polish writer is subjective and can vary depending on personal opinions and literary critiques. However, one writer who is often regarded as one of the greatest Polish writers is Adam Mickiewicz (1798-1855).\\n\\nMickiewicz is a Polish poet, playwright, and novelist, best known for his epic poem \\\"Pan Tadeusz\\\" (1834), which is considered Poland's national epic. The poem tells the story of the Polish-Lithuanian Commonwealth and its struggle for independence against the Russian Empire.\\n\\nMickiewicz is also known for his other works, such as \\\"Dziady\\\" (Forefathers' Eve), a play that explores the themes of Polish identity, history, and culture. He is considered one of the most important figures in Polish literature and a key figure in the Polish Romantic movement.\\n\\nOther notable Polish writers who are often considered among the greatest include:\\n\\n* Henryk Sienkiewicz (1846-1916), a Nobel Prize-winning novelist known for his historical novels, such as \\\"Quo Vadis\\\" (1896) and \\\"The Teutonic Knights\\\" (1900).\\n* Juliusz Słowacki (1809-1849), a poet and playwright who is considered one of the most important figures in Polish Romanticism.\\n* Cyprian Kamil Norwid (1821-1883), a poet, playwright, and painter who is known for his innovative and expressive style.\\n* Wisława Szymborska (1923-2012), a Nobel Prize-winning poet who is known for her concise and insightful poetry.\\n\\nOverall, while opinions may vary, Adam Mickiewicz is often regarded as one of the greatest Polish writers, and his works continue to be widely read and studied in Poland and around the world.\", \"role\": \"assistant\" }, \"finish_reason\": \"stop\", \"index\": 0 } ] } And here is Amazon’s Nova Micro:\n{ \"model\": \"us.amazon.nova-micro-v1:0\", \"usage\": { \"prompt_tokens\": 8, \"completion_tokens\": 109, \"total_tokens\": 117 }, \"object\": \"chat.completion\", \"choices\": [ { \"message\": { \"content\": \"The greatest Polish writer is often considered to be **Henryk Sienkiewicz**. He is best known for his historical novel \\\"Quo Vadis,\\\" which earned him the Nobel Prize in Literature in 1905. Sienkiewicz also wrote other notable works, including \\\"With Fire and Sword\\\" and \\\"The Deluge,\\\" which are part of his series known as \\\"The Trilogy.\\\" His works are celebrated for their vivid portrayal of Polish history and culture, and they continue to be widely read and appreciated both in Poland and internationally.\", \"role\": \"assistant\" }, \"finish_reason\": \"stop\", \"index\": 0 } ] }",
    "description": "Let’s get started with a simple round-robin policy:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:\nbedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: balancer: algorithm: round-robin targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy-advanced.yaml :::",
    "tags": [],
    "title": "Round Robin",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/round-robin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e Prompt Engineering",
    "content": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.\nFor llm/v1/chat type models: You can optionally configure the plugin to ignore existing chat history, wherein it will only scan the trailing user message. For llm/v1/completions type models: There is only one prompt field, thus the whole prompt is scanned on every request. The plugin matches lists of regular expressions to requests through AI Proxy. The matching behavior is as follows:\nIf any deny expressions are set, and the request matches any regex pattern in the deny list, the caller receives a 400 response. If any allow expressions are set, but the request matches none of the allowed expressions, the caller also receives a 400 response. If any allow expressions are set, and the request matches one of the allow expressions, the request passes through to the LLM. If there are both deny and allow expressions set, the deny condition takes precedence over allow. Any request that matches an entry in the deny list will return a 400 response, even if it also matches an expression in the allow list. If the request does not match an expression in the deny list, then it must match an expression in the allow list to be passed through to the LLM Here’s an example to allow only valid credit cards numbers:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-prompt-guard.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nbedrock services: name: service1 host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy instance_name: “ai-proxy-bedrock” config: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: “llm/v1/chat” model: provider: “bedrock” options: bedrock: aws_region: “us-west-2” name: ai-prompt-guard instance_name: ai-prompt-guard-bedrock enabled: true config: allow_all_conversation_history: true allow_patterns: “.\\\"card\\\".\\\"4[0-9]{3}\\*{12}\\\"” EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-prompt-guard.yaml :::\nSend a request with a valid credit card pattern:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data-raw ‘{ “messages”: [ { “role”: “user”, “content”: “Validate this card: {\"card\": \"4111************\", \"cvv\": \"000\"}” } ], “model”: “us.amazon.nova-lite-v1:0” }’ | jq ‘.’ :::\nNow, send a non-valid number:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data-raw ‘{ “messages”: [ { “role”: “user”, “content”: “Validate this card: {\"card\": \"4111xyz************\", \"cvv\": \"000\"}” } ], “model”: “us.amazon.nova-lite-v1:0” }’ | jq ‘.’ :::\nThe expect result is:\n{ \"error\": { \"message\": \"bad request\" } }",
    "description": "The AI Prompt Guard plugin lets you to configure a series of PCRE-compatible regular expressions as allow or deny lists, to guard against misuse of llm/v1/chat or llm/v1/completions requests.\nYou can use this plugin to allow or block specific prompts, words, phrases, or otherwise have more control over how an LLM service is used when called via Kong Gateway. It does this by scanning all chat messages (where the role is user) for the specific expressions set. You can use a combination of allow and deny rules to preserve integrity and compliance when serving an LLM service using Kong Gateway.",
    "tags": [],
    "title": "AI Prompt Guard plugin",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/prompt-guard/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced plugin",
    "content": "Now, let’s redirect 80% of the request to Amazon’s Nova with a weight based policy:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:\nbedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false weight: 80 model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false weight: 20 EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy-advanced.yaml :::",
    "description": "Now, let’s redirect 80% of the request to Amazon’s Nova with a weight based policy:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:\nbedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false weight: 80 model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false weight: 20 EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy-advanced.yaml :::",
    "tags": [],
    "title": "Weight",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/weight/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced plugin",
    "content": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:\nbedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false EOF ::: Apply the declaration with decK:\n:::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy-advanced.yaml :::\nTest the Route again.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is considered the greatest Polish writer?” } ] }’ | jq :::\nLowest Usage policy The lowest-usage algorithm in AI Proxy Advanced is based on the volume of usage for each model. It balances the load by distributing requests to models with the lowest usage, measured by factors such as prompt token counts, response token counts, or other resource metrics.\nReplace the declaration:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:\nbedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: balancer: algorithm: lowest-usage targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false EOF ::: Apply the declaration:\n:::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy-advanced.yaml :::\nAnd test the Route again.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is considered the greatest Polish writer?” } ] }’ | jq :::",
    "description": "Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.\nCreate a file with the following declaration:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:\nbedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false EOF ::: Apply the declaration with decK:",
    "tags": [],
    "title": "Lowest-Latency and Lowest-Usage",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/lowest-latency-usage/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases \u003e AI Proxy Advanced plugin",
    "content": "Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:\nbedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: balancer: algorithm: semantic embeddings: auth: param_name: “allow_override” param_value: “false” param_location: “body” model: provider: bedrock name: “amazon.titan-embed-text-v2:0” options: bedrock: aws_region: us-west-2 vectordb: dimensions: 1024 distance_metric: cosine strategy: redis threshold: 1.0 redis: host: “redis-stack.redis.svc.cluster.local” port: 6379 database: 0 targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false description: “mathematics, algebra, calculus, trigonometry” model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false description: “piano, orchestra, liszt, classical music” EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy-advanced.yaml :::\nSend a request related to Mathematics. The response should come from Amazon’s Nova Micro model :::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Tell me about the last theorem of Fermat” } ] }’ | jq :::\nOn the other hand, Llama3.3 should be responsible for requests related to Classical Music.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who wrote the Hungarian Rhapsodies piano pieces?” } ] }’ | jq :::\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Tell me a contemporaty pianist of Chopin” } ] }’ | jq :::\nIf you check Redis, you’ll se there are two entries, related to the models :::code{showCopyAction=true showLineNumbers=false language=shell} kubectl exec -it $(kubectl get pod -n redis -o json | jq -r ‘.items[].metadata.name’) -n redis – redis-cli –scan :::\nExpected output \"ai_proxy_advanced_semantic:01c84f59-b7c3-418b-818d-4369ef3e55ef:8f74aeaab95482bb37fbd69cd42154dcd6d321e1631ffdfd1802e1609d4c2481\" \"ai_proxy_advanced_semantic:01c84f59-b7c3-418b-818d-4369ef3e55ef:72a33ce9079fd34f6fb3624c3a4ba1a0df0c1aad267986db2249dc26a8808a41\"",
    "description": "Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.",
    "tags": [],
    "title": "Semantic Routing",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/semantic-routing/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "This chapter will walk you through the pre-requisites.\nKong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.\nIf you want to check the Konnect Pricing and Plans, please, redirect to https://konghq.com/pricing\nCommand Line Utilities In this workshop, we will use the following command line utilities\nkubectl (we will install this in subsequent section) curl (pre-installed) jq (we will install this in subsequent section)",
    "description": "This chapter will walk you through the pre-requisites.\nKong Konnect Subscription You will need a Kong Konnect Subscription to execute this workshop.\nIf you want to check the Konnect Pricing and Plans, please, redirect to https://konghq.com/pricing\nCommand Line Utilities In this workshop, we will use the following command line utilities\nkubectl (we will install this in subsequent section) curl (pre-installed) jq (we will install this in subsequent section)",
    "tags": [],
    "title": "Pre-Requisites",
    "uri": "/10-pre-requisites/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Pre-Requisites",
    "content": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.\nClick on the Registration link and present your credentials.\nKonnect will send you an email to confirm the subscription. Click on the link in email to confirm your subscription.\nThe Konnect environment can be accessed via the Konnect log in page.\nAfter logging in create an organisation name, select a region, then answer a few questions.\nCredit available can be monitored though Plan and Usage page.",
    "description": "Konnect Plus Subscription You will need a Konnect subscription. Follow the steps below to obtain a Konnect Plus subscription. Initially $500 of credits are provided for up to 30 days, after the credits have expired or the time has finished Konnect Plus will charge based on Konnect Plus pricing. Following the workshop instructions will use less than $500 credits. After the workshop has finished the Konnect Plus environment can be deleted.",
    "tags": [],
    "title": "Konnect Subscription",
    "uri": "/10-pre-requisites/konnect-control-plane/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Pre-Requisites",
    "content": "Access to OpenAI foundation models isn’t granted by default. In order to gain access to the foundational model, follow the steps below.\nChoose the following models:\nAmazon: Nova Lite - amazon.nova-lite-v1:0 Nova Micro - amazon.nova-micro-v1:0 Titan Text Embeddings V2 - amazon.titan-embed-text-v2:0 Meta: Llama 3.3 70B Instruct v1 - meta.llama3-3-70b-instruct-v1:0 This wraps up the model access configuration, in the next section you will learn how to use these models in the application.",
    "description": "Access to OpenAI foundation models isn’t granted by default. In order to gain access to the foundational model, follow the steps below.\nChoose the following models:\nAmazon: Nova Lite - amazon.nova-lite-v1:0 Nova Micro - amazon.nova-micro-v1:0 Titan Text Embeddings V2 - amazon.titan-embed-text-v2:0 Meta: Llama 3.3 70B Instruct v1 - meta.llama3-3-70b-instruct-v1:0 This wraps up the model access configuration, in the next section you will learn how to use these models in the application.",
    "tags": [],
    "title": "OpenAI model access",
    "uri": "/10-pre-requisites/openai-model-access/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "This chapter will walk you through\nKonnect Control Plane and Data Plane creation using Kong Gateway Operator (KGO). Scale Kong data plane nodes on Kubernetes using HPA - Horizontal Pod Autoscaler. Access Kong data plane through Minikube tunnel Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your application micros surfaces or services. Konnect Control Plane After Konnect registration, you need to create your first Control Plane. There are multiple ways to do it:\nKonnect User Interface. RESTful Admin API, a fundamental mechanism for administration purposes. Kong Gateway Operator (KGO) and Kubernetes CRDs To get an easier and faster deployment, this workshop uses KGO. You may observe the output in Konnect UI.\nThis tutorial is intended to be used for labs and PoC only. There are many aspects and processes, typically implemented in production sites, not described here. For example: Digital Certificate issuing, Cluster monitoring, etc. For a production ready deployment, refer Kong on Terraform Constructs, available here\nYou can now click Next to begin the module.",
    "description": "This chapter will walk you through\nKonnect Control Plane and Data Plane creation using Kong Gateway Operator (KGO). Scale Kong data plane nodes on Kubernetes using HPA - Horizontal Pod Autoscaler. Access Kong data plane through Minikube tunnel Here’s a Reference Architecture that will be implemented in this workshop:\nKong Konnect Control Plane: responsible for managing your APIs Kong Konnect Data Plane: connected to the Control Plane, it is responsible for processing all the incoming requests sent by the consumers. Kong provides a plugin framework, where each one of them is responsible for a specific functionality. As a can see, there are two main collections of plugins: On the left, the historic and regular API Gateway plugins, implementing all sort of policies including, for example, OIDC based Authentication processes with Keycloak, Amazon Cognito and Okta or Observability with Prometheus/Grafana and Dynatrace. On the right, another plugin collection for AI-based use cases. For example, the AI Rate Limiting plugin implements policies like this based on the number of tokens consumed be the requests. Or, as another example is the AI Semantic Cache plugin, which caches data based on the semantics related to the responses coming from the LLM models. Kong AI Gateway supports, out of the box, a variety of infrastructures, including not just OpenAI, but also Amazon Bedrock, Google Gemini, Mistral, Anthropic, etc. In order to deal with embeddings, the Gateway also supports also vector databases. Kong Gateway protects not just the LLM Models but also the upstream services, including your application micros surfaces or services. Konnect Control Plane After Konnect registration, you need to create your first Control Plane. There are multiple ways to do it:",
    "tags": [],
    "title": "Konnect Setup",
    "uri": "/11-konnect-setup/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Konnect Setup",
    "content": "We are going to deploy our Data Plane in a Minikube Cluster over Podman. You can start Podman with:\npodman machine set --memory 8196 podman machine start If you want to stop it run: podman machine stop Then you can install Minikube with: minikube start --driver=podman --memory='no-limit' Use should see your cluster running with: kubectl get all --all-namespaces Typical output is:\nNAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-674b8bbfcf-xrllp 0/1 Running 0 12s kube-system pod/etcd-minikube 1/1 Running 0 18s kube-system pod/kube-apiserver-minikube 1/1 Running 0 18s kube-system pod/kube-controller-manager-minikube 1/1 Running 0 18s kube-system pod/kube-proxy-xkfn9 1/1 Running 0 13s kube-system pod/kube-scheduler-minikube 1/1 Running 0 18s kube-system pod/storage-provisioner 1/1 Running 0 17s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 19s kube-system service/kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 18s NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-proxy 1 1 1 1 1 kubernetes.io/os=linux 18s NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 0/1 1 0 18s NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-674b8bbfcf 1 1 0 12s To be able to consume the Kubernetes Load Balancer Services, in another terminal run: minikube tunnel You can now click Next to install the operator.",
    "description": "We are going to deploy our Data Plane in a Minikube Cluster over Podman. You can start Podman with:\npodman machine set --memory 8196 podman machine start If you want to stop it run: podman machine stop Then you can install Minikube with: minikube start --driver=podman --memory='no-limit' Use should see your cluster running with: kubectl get all --all-namespaces Typical output is:\nNAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-674b8bbfcf-xrllp 0/1 Running 0 12s kube-system pod/etcd-minikube 1/1 Running 0 18s kube-system pod/kube-apiserver-minikube 1/1 Running 0 18s kube-system pod/kube-controller-manager-minikube 1/1 Running 0 18s kube-system pod/kube-proxy-xkfn9 1/1 Running 0 13s kube-system pod/kube-scheduler-minikube 1/1 Running 0 18s kube-system pod/storage-provisioner 1/1 Running 0 17s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 19s kube-system service/kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 18s NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-proxy 1 1 1 1 1 kubernetes.io/os=linux 18s NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 0/1 1 0 18s NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-674b8bbfcf 1 1 0 12s To be able to consume the Kubernetes Load Balancer Services, in another terminal run: minikube tunnel",
    "tags": [],
    "title": "Minikube",
    "uri": "/11-konnect-setup/110-minikube/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Konnect Setup",
    "content": "KGO requires a Konnect Personal Access Token (PAT) for creating the Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it, as Konnect won’t display it again.\nNote Be sure to copy and save your PAT, as Konnect won’t display it again.\nKonnect PAT secret Create a Kubernetes (K8) Secret with your PAT in the kong namespace. KGO requires the secret to be labeled.\nSave PAT in an environment variables export PAT=PASTE_THE_CONTENTS_OF_COPIED_PAT Create the namespace kubectl create namespace kong Create K8s Secret with PAT\nNote Pls don’t forget to replace PASTE_THE_CONTENTS_OF_COPIED_PAT in the below command with the copied PAT from Kong UI.\nkubectl create secret generic konnect-pat -n kong --from-literal=token=$(echo $PAT) kubectl label secret konnect-pat -n kong \"konghq.com/credential=konnect\" Check your Secret. You should your PAT. kubectl get secret konnect-pat -n kong -o jsonpath='{.data.*}' | base64 -d You can now click Next to install the operator.",
    "description": "KGO requires a Konnect Personal Access Token (PAT) for creating the Control Plane. To generate your PAT, click on your initials in the upper right corner of the Konnect home page, then select Personal Access Tokens. Click on + Generate Token, name your PAT, set its expiration time, and be sure to copy and save it, as Konnect won’t display it again.",
    "tags": [],
    "title": "PAT - Personal Access Token",
    "uri": "/11-konnect-setup/111-personal-access-token/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Konnect Setup",
    "content": "Install the Operator To get started let’s install the Operator:\nhelm repo add kong https://charts.konghq.com helm repo update kong helm upgrade --install kgo kong/gateway-operator \\ -n kong-system \\ --create-namespace \\ --set image.tag=1.6.2 \\ --set kubernetes-configuration-crds.enabled=true \\ --set env.ENABLE_CONTROLLER_KONNECT=true You can check the Operator’s log with: kubectl logs $(kubectl get pod -n kong-system -o json | jq -r '.items[].metadata | select(.name | startswith(\"kgo-gateway-operator\"))' | jq -r '.name') -n kong-system Kong-gratulations! have now reached the end of this module by creating a Kong Operator. You can now click Next to proceed with the next module.",
    "description": "Install the Operator To get started let’s install the Operator:\nhelm repo add kong https://charts.konghq.com helm repo update kong helm upgrade --install kgo kong/gateway-operator \\ -n kong-system \\ --create-namespace \\ --set image.tag=1.6.2 \\ --set kubernetes-configuration-crds.enabled=true \\ --set env.ENABLE_CONTROLLER_KONNECT=true You can check the Operator’s log with: kubectl logs $(kubectl get pod -n kong-system -o json | jq -r '.items[].metadata | select(.name | startswith(\"kgo-gateway-operator\"))' | jq -r '.name') -n kong-system Kong-gratulations! have now reached the end of this module by creating a Kong Operator. You can now click Next to proceed with the next module.",
    "tags": [],
    "title": "Kong Gateway Operator",
    "uri": "/11-konnect-setup/112-kgo-installation/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Konnect Setup",
    "content": "Control Plane Deployment The following declaration defines an Authentication Configuration, based on the Kubernetes Secret and referring to a Konnect API URL, and the actual Konnect Control Plane.\ncat \u003c\u003cEOF | kubectl apply -f - kind: KonnectAPIAuthConfiguration apiVersion: konnect.konghq.com/v1alpha1 metadata: name: konnect-api-auth-conf namespace: kong spec: type: secretRef secretRef: name: konnect-pat namespace: kong serverURL: us.api.konghq.com --- kind: KonnectGatewayControlPlane apiVersion: konnect.konghq.com/v1alpha1 metadata: name: kong-workshop namespace: kong spec: name: kong-workshop konnect: authRef: name: konnect-api-auth-conf EOF Expected Sample Output\nkonnectapiauthconfiguration.konnect.konghq.com/konnect-api-auth-conf created konnectgatewaycontrolplane.konnect.konghq.com/kong-aws created If you go to Konnect UI \u003e Gateway manager, you should see a new control plane named kong-aws getting created.\nData Plane deployment The next declaration instantiates a Data Plane connected to your Control Plane. It creates a KonnectExtension, asking KGO to manage the certificate and private key provisioning automatically, and the actual Data Plane. The Data Plane declaration specifies the Docker image, in our case 3.11, as well as how the Kubernetes Service, related to the Data Plane, should be created. Also, we use the the Data Plane deployment refers to the Kubernetes Service Account we created before.\ncat \u003c\u003cEOF | kubectl apply -f - kind: KonnectExtension apiVersion: konnect.konghq.com/v1alpha1 metadata: name: konnect-config1 namespace: kong spec: clientAuth: certificateSecret: provisioning: Automatic konnect: controlPlane: ref: type: konnectNamespacedRef konnectNamespacedRef: name: kong-workshop --- apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.11 network: services: ingress: name: proxy1 type: LoadBalancer EOF It takes some minutes to get the Load Balancer provisioned and avaiable. Get its domain name with:\nexport DATA_PLANE_LB=$(kubectl get svc -n kong proxy1 --output=jsonpath='{.status.loadBalancer.ingress[].ip}') View the load balancer DNS as\necho $DATA_PLANE_LB Try calling it as\ncurl -w '\\n' $DATA_PLANE_LB Expected Output\n{ \"message\":\"no Route matched with those values\", \"request_id\":\"d364362a60b32142fed73712a9ea1948\" } Further Reading Kong Konnect API auth configuration Kong-gratulations! have now reached the end of this module by creating control plane and data plane. You can now click Next to proceed with the next module.",
    "description": "Control Plane Deployment The following declaration defines an Authentication Configuration, based on the Kubernetes Secret and referring to a Konnect API URL, and the actual Konnect Control Plane.\ncat \u003c\u003cEOF | kubectl apply -f - kind: KonnectAPIAuthConfiguration apiVersion: konnect.konghq.com/v1alpha1 metadata: name: konnect-api-auth-conf namespace: kong spec: type: secretRef secretRef: name: konnect-pat namespace: kong serverURL: us.api.konghq.com --- kind: KonnectGatewayControlPlane apiVersion: konnect.konghq.com/v1alpha1 metadata: name: kong-workshop namespace: kong spec: name: kong-workshop konnect: authRef: name: konnect-api-auth-conf EOF Expected Sample Output",
    "tags": [],
    "title": "Control Plane and Data Plane",
    "uri": "/11-konnect-setup/113-cp-dp/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Konnect Setup",
    "content": "One of the most important capabilities provided by Kubernetes is to easily scale out a Deployment. With a single command we can create or terminate pod replicas in order to optimally support a given throughput.\nThis capability is especially interesting for Kubernetes applications like Kong for Kubernetes Ingress Controller.\nHere’s our deployment before scaling it out:\nkubectl get service -n kong Sample Output\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dataplane-admin-dataplane1-x9n6r ClusterIP None \u003cnone\u003e 8444/TCP 20h proxy1 LoadBalancer 10.100.234.223 k8s-kong-proxy1-518c8abdcc-a10b0ef08ae8ba02.elb.us-east-2.amazonaws.com 80:32290/TCP,443:31084/TCP 20h Notice, at this point in the workshop, there is only one pod taking data plane traffic.\nkubectl get pod -n kong -o wide Sample Output\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dataplane-dataplane1-ch6g9-6889fdf76b-5gjh9 1/1 Running 0 20h 192.168.61.40 ip-192-168-44-68.us-east-2.compute.internal \u003cnone\u003e \u003cnone\u003e Manual Scaling Out Now, let’s scale the deployment out creating 3 replicas of the pod\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.10.0.1 serviceAccountName: kaigateway-podid-sa replicas: 3 network: services: ingress: name: proxy1 type: LoadBalancer annotations: \"service.beta.kubernetes.io/aws-load-balancer-scheme\": \"internet-facing\" \"service.beta.kubernetes.io/aws-load-balancer-nlb-target-type\": \"ip\" EOF Check the Deployment again and now you should see 3 replicas of the pod.\n:::code{showCopyAction=true showLineNumbers=false language=shell} kubectl get pod -n kong -o wide :::\nSample Output\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dataplane-dataplane1-ch6g9-6889fdf76b-5gjh9 1/1 Running 0 20h 192.168.61.40 ip-192-168-44-68.us-east-2.compute.internal \u003cnone\u003e \u003cnone\u003e dataplane-dataplane1-ch6g9-6889fdf76b-9gt4s 1/1 Running 0 6m15s 192.168.52.45 ip-192-168-44-68.us-east-2.compute.internal \u003cnone\u003e \u003cnone\u003e dataplane-dataplane1-ch6g9-6889fdf76b-mrjdx 1/1 Running 0 6m15s 192.168.36.12 ip-192-168-44-68.us-east-2.compute.internal \u003cnone\u003e \u003cnone\u003e As we can see, the 2 new Pods have been created and are up and running. If we check our Kubernetes Service again, we will see it has been updated with the new IP addresses. That allows the Service to implement Load Balancing across the Pod replicas.\n:::code{showCopyAction=true showLineNumbers=false language=shell} kubectl describe service proxy1 -n kong :::\nSample Output\nName: proxy1 Namespace: kong Labels: app=dataplane1 gateway-operator.konghq.com/dataplane-service-state=live gateway-operator.konghq.com/dataplane-service-type=ingress gateway-operator.konghq.com/managed-by=dataplane Annotations: gateway-operator.konghq.com/last-applied-annotations: {\"service.beta.kubernetes.io/aws-load-balancer-nlb-target-type\":\"ip\",\"service.beta.kubernetes.io/aws-load-balancer-scheme\":\"internet-facin... service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing Selector: app=dataplane1,gateway-operator.konghq.com/selector=2231a2f4-f440-4453-98a4-872ed899169b Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.100.234.223 IPs: 10.100.234.223 LoadBalancer Ingress: k8s-kong-proxy1-518c8abdcc-a10b0ef08ae8ba02.elb.us-east-2.amazonaws.com Port: http 80/TCP TargetPort: 8000/TCP NodePort: http 32290/TCP Endpoints: 192.168.61.40:8000,192.168.36.12:8000,192.168.52.45:8000 Port: https 443/TCP TargetPort: 8443/TCP NodePort: https 31084/TCP Endpoints: 192.168.61.40:8443,192.168.36.12:8443,192.168.52.45:8443 Session Affinity: None External Traffic Policy: Cluster Internal Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfullyReconciled 13m (x2 over 20h) service Successfully reconciled Reduce the number of Pods to 1 again running as now we will turn on Horizontal pod autoscalar.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.10.0.1 serviceAccountName: kaigateway-podid-sa replicas: 1 network: services: ingress: name: proxy1 type: LoadBalancer annotations: \"service.beta.kubernetes.io/aws-load-balancer-scheme\": \"internet-facing\" \"service.beta.kubernetes.io/aws-load-balancer-nlb-target-type\": \"ip\" EOF HPA - Horizontal Autoscaler HPA (“Horizontal Pod Autoscaler”) is the Kubernetes resource to automatically control the number of replicas of Pods. With HPA, Kubernetes is able to support the requests produced by the consumers, keeping a given Service Level.\nBased on CPU utilization or custom metrics, HPA starts and terminates Pods replicas updating all service data to help on the load balancing policies over those replicas.\nHPA is described at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/. Also, there’s a nice walkthrough at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/\nKubernetes defines its own units for cpu and memory. You can read more about it at: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/. We use these units to set our Deployments with HPA.\nMetrics Server HPA relies no the Metrics Server to control the number of replicas of a given deployment. Check it as follows:\nkubectl get pod -n kube-system -o json | jq -r '.items[].metadata | select(.name | startswith(\"metrics-server-\"))' | jq -r '.name' Now you should see two metrics-server- pods in Running state\nSample Output\nmetrics-server-84cbf4fd8-9fblt metrics-server-84cbf4fd8-zw6hs Turn HPA on Still using the Operator, let’s upgrade our Data Plane deployment including new and specific settings for HPA. The new settings are defining the ammount of CPU and memory each Pod should allocate. At the same time, the “scaling” sets are telling HPA how to proceed to instantiate new Pod replicas.\nHere’s the final declaration:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.10.0.1 resources: requests: memory: \"300Mi\" cpu: \"300m\" limits: memory: \"800Mi\" cpu: \"1200m\" serviceAccountName: kaigateway-podid-sa scaling: horizontal: minReplicas: 1 maxReplicas: 20 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 20 EOF Checking HPA After submitting the command check the Deployment again. Since we’re not consume the Data Plane, we are supposed to see a single Pod running. In the next sections we’re going to send requests to the Data Plane and new Pod will get created to handle them.\nkubectl get pod -n kong Sample Output\nNAME READY STATUS RESTARTS AGE dataplane-dataplane1-ch6g9-5fb9c6484b-kklw5 1/1 Running 0 73s You can check the HPA status with:\nkubectl get hpa -n kong Send traffic\nWe are going to use Fortio to consume the Data Plane and see the HPA in action. Note we are interested on sending traffic to the Data Plane only, so we are consuming a non-existing Route.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: fortio labels: app: fortio spec: containers: - name: fortio image: fortio/fortio args: [\"load\", \"-c\", \"800\", \"-qps\", \"3000\", \"-t\", \"20m\", \"-allow-initial-errors\", \"http://proxy1.kong.svc.cluster.local:80/route1/get\"] EOF Sample Output\nEventually, HPA will start a new replica:\n% kubectl get hpa -n kong NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE dataplane1 Deployment/dataplane-dataplane1-ch6g9 cpu: 2%/75% 1 20 2 14m % kubectl get pod -n kong -o json | jq -r '.items[].metadata.name' dataplane-dataplane1-ch6g9-8c6c9cfcd-qbqp5 dataplane-dataplane1-ch6g9-8c6c9cfcd-rxcwm If you delete the Fortio pod, HPA should terminate one pod and get back to 1 replica only. kubectl delete pod fortio Delete HPA\nDelete the HPA setting applying the original declaration\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions: - kind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.10.0.1 serviceAccountName: kaigateway-podid-sa replicas: 1 network: services: ingress: name: proxy1 type: LoadBalancer annotations: \"service.beta.kubernetes.io/aws-load-balancer-scheme\": \"internet-facing\" \"service.beta.kubernetes.io/aws-load-balancer-nlb-target-type\": \"ip\" EOF Kong-gratulations! have now reached the end of this module by implementing and successfully testing Horizontal Pod AutoScaling. You can now click Next to proceed with the next chapter.",
    "description": "One of the most important capabilities provided by Kubernetes is to easily scale out a Deployment. With a single command we can create or terminate pod replicas in order to optimally support a given throughput.\nThis capability is especially interesting for Kubernetes applications like Kong for Kubernetes Ingress Controller.\nHere’s our deployment before scaling it out:\nkubectl get service -n kong Sample Output\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dataplane-admin-dataplane1-x9n6r ClusterIP None \u003cnone\u003e 8444/TCP 20h proxy1 LoadBalancer 10.100.234.223 k8s-kong-proxy1-518c8abdcc-a10b0ef08ae8ba02.elb.us-east-2.amazonaws.com 80:32290/TCP,443:31084/TCP 20h Notice, at this point in the workshop, there is only one pod taking data plane traffic.",
    "tags": [],
    "title": "Data Plane Elasticity",
    "uri": "/11-konnect-setup/114-elasticity/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nDeploy an application to get protected by the Data Plane Use decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.\ndecK operates on state files. decK state files describe the configuration of Kong API Gateway. State files encapsulate the complete configuration of Kong in a declarative format, including services, routes, plugins, consumers, and other entities that define how requests are processed and routed through Kong.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Konnect Gateway Manager",
    "description": "With our Control Plane created and Data Plane layer deployed it’s time to create an API and expose an application. In this module, we will:\nDeploy an application to get protected by the Data Plane Use decK to define a Kong Service based on an endpoint provided by the application and a Kong Route on top of the Kong Service to expose the application. Enable Kong Plugins to the Kong Route or Kong Service. Define Kong Consumers to represent the entities sending request to the Gateway and enable Kong Plugin to them. With decK (declarations for Kong) you can manage Kong Konnect configuration in a declaratively way.",
    "tags": [],
    "title": "Kong API Gateway",
    "uri": "/12-api-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway",
    "content": "For the purpose of this workshop, you’ll create and expose a service to the HTTPbin API. HTTPbin is an echo-type application that returns requests back to the requester as responses.\nDeploy the Application Deploy the application using the following declaration with both Kubernetes Deployment and Service.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: httpbin namespace: kong labels: app: httpbin spec: type: ClusterIP ports: - name: http port: 8000 targetPort: 80 selector: app: httpbin --- apiVersion: apps/v1 kind: Deployment metadata: name: httpbin namespace: kong spec: replicas: 1 selector: matchLabels: app: httpbin version: v1 template: metadata: labels: app: httpbin version: v1 spec: containers: - image: docker.io/kong/httpbin imagePullPolicy: IfNotPresent name: httpbin ports: - containerPort: 8000 EOF Check the Deployment Observe Kubernetes Services in kong namespace kubectl get service httpbin -n kong Sample Output\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE httpbin ClusterIP 10.100.89.150 \u003cnone\u003e 8000/TCP 20h Observe Pods in kong namespace kubectl get pod -n kong Sample Output\nNAME READY STATUS RESTARTS AGE dataplane-dataplane1-rbwck-98fcbc654-rt75p 1/1 Running 0 76m httpbin-5c69574c95-xq76q 1/1 Running 0 20h Ping Konnect with decK Before start using decK, you should ping Konnect to check if the connecting is up. Note we assume you have the PAT environment variable set. Please, refer to the previous section to learn how to issue a PAT.\ndeck gateway ping --konnect-control-plane-name kong-workshop --konnect-token $PAT Expected Output\nSuccessfully Konnected to the Example-Name organization! Create a Kong Gateway Service and Kong Route Create the following declaration first. Remarks:\nNote the host and port refers to the HTTPbin’s Kubernetes Service FQDN (Fully Qualified Domain Name), in our case http://httpbin.kong.svc.cluster.local:8000. The declaration tags the objects so you can managing them apart from other ones. cat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route paths: - /httpbin-route EOF Submit the declaration Now, you can use the following command to sync your Konnect Control Plane with the declaration. Note that all other existing objects will be deleted.\ndeck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating service httpbin-service creating route httpbin-route Summary: Created: 2 Updated: 0 Deleted: 0 You should see your new service’s overview page.\nConsume the Route We are to use the same ELB provisioned during the Data Plane deployment:\ncurl -v $DATA_PLANE_LB/httpbin-route/get If successful, you should see the httpbin output:\n* Trying 127.0.0.1:80... * Connected to 127.0.0.1 (127.0.0.1) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: 127.0.0.1 \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Content-Length: 377 \u003c Connection: keep-alive \u003c Server: gunicorn \u003c Date: Wed, 06 Aug 2025 16:19:15 GMT \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Credentials: true \u003c X-Kong-Upstream-Latency: 6 \u003c X-Kong-Proxy-Latency: 3 \u003c Via: 1.1 kong/3.11.0.2-enterprise-edition \u003c X-Kong-Request-Id: 0cbe555eefb4f14bb43f9b511435bd5c \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"127.0.0.1\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"0cbe555eefb4f14bb43f9b511435bd5c\"},\"origin\":\"10.244.0.1\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host 127.0.0.1 left intact Kong-gratulations! have now reached the end of this module by having your first service set up, running, and routing traffic proxied through a Kong data plane. You can now click Next to proceed with the next module.\nKong-gratulations! have now reached the end of this module by creating a Kong Service. You can now click Next to proceed with the next chapter.",
    "description": "For the purpose of this workshop, you’ll create and expose a service to the HTTPbin API. HTTPbin is an echo-type application that returns requests back to the requester as responses.\nDeploy the Application Deploy the application using the following declaration with both Kubernetes Deployment and Service.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: httpbin namespace: kong labels: app: httpbin spec: type: ClusterIP ports: - name: http port: 8000 targetPort: 80 selector: app: httpbin --- apiVersion: apps/v1 kind: Deployment metadata: name: httpbin namespace: kong spec: replicas: 1 selector: matchLabels: app: httpbin version: v1 template: metadata: labels: app: httpbin version: v1 spec: containers: - image: docker.io/kong/httpbin imagePullPolicy: IfNotPresent name: httpbin ports: - containerPort: 8000 EOF Check the Deployment Observe Kubernetes Services in kong namespace kubectl get service httpbin -n kong Sample Output",
    "tags": [],
    "title": "Kong Gateway Service and Kong Route",
    "uri": "/12-api-gateway/122-kong-service-route/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:\nGetting Started with Amazon Bedrock and Kong AI Gateway We are going to get started with a simple configuration. The following decK declaration enables the AI Proxy plugin to the Kong Gateway Service, to send requests to Amazon Bedrock and consume the amazon.nova-micro-v1:0 Amazon FM with chat LLM requests.\nFor Amazon Bedrock, the allow_override parameter should be set to false, saying the the authorization header or parameter can not be overridden by requests.\nUpdate your ai-gateway.yaml file with that:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nbedrock services: name: service1 host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy instance_name: “ai-proxy-bedrock” config: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: “llm/v1/chat” model: provider: “bedrock” name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: “us-west-2” EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy.yaml :::\nOpenAI API Kong AI Gateway provides one API to access all of the LLMs it supports. To accomplish this, Kong AI Gateway has standardized on the OpenAI API specification. This will help developers to onboard more quickly by providing them with an API specification that they’re already familiar with. You can start using LLMs behind the AI Gateway simply by redirecting your requests to a URL that points to a route of the AI Gateway.\nSend a request to Kong AI Gateway Now, send a request to Kong AI Gateway following the OpenAI API Chat specification as a reference:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “what is pi?” } ] }’ | jq :::\nExpected Output\nNote the response also complies to the OpenAI API spec:\n{ \"object\": \"chat.completion\", \"model\": \"us.amazon.nova-micro-v1:0\", \"usage\": { \"prompt_tokens\": 4, \"total_tokens\": 187, \"completion_tokens\": 183 }, \"choices\": [ { \"message\": { \"role\": \"assistant\", \"content\": \"Pi (π) is a mathematical constant that represents the ratio of a circle's circumference to its diameter. It is an irrational number, which means it cannot be expressed exactly as a simple fraction and its decimal representation goes on infinitely without repeating. The value of pi is approximately 3.14159, but it is commonly rounded to 3.14 for simpler calculations.\\n\\nPi is used in various fields of mathematics and science, especially in geometry and trigonometry, to solve problems related to circles, spheres, and other circular or spherical shapes. Its importance spans across numerous applications, from engineering and physics to computer science and statistics.\\n\\nIn mathematical notation, pi is represented by the Greek letter π, and it has a numerical value of:\\n\\n\\\\[ \\\\pi \\\\approx 3.141592653589793 \\\\]\\n\\n(and so on).\" }, \"index\": 0, \"finish_reason\": \"stop\" } ] } AI Proxy configuration parameters The AI Proxy plugin is responsible for a variety of topics. For example:\nRequest and response formats appropriate for the configured provider and route_type settings. The route_type AI Proxy configuration parameter defines which kind of request the AI Gateway is going to perform. It must be one of: llm/v1/chat, llm/v1/completions or preserve. Set to preserve to pass through without transformation. Authentication on behalf of the Kong API consumer. Decorating the request with parameters from the config.model.options block, appropriate for the chosen provider. For our case, we tell which AWS region we want to send requests to. Define the model to be consume when sending the request As you may have noticed our AI Proxy plugin defines the Bedrock model it should consume. That is can be done for individual requests, if required. Change the ai-gateway.yaml file, removing the model’s name parameter and apply the declaration again:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-proxy.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nbedrock services: name: service1 host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy instance_name: “ai-proxy-bedrock” config: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: “llm/v1/chat” model: provider: “bedrock” options: bedrock: aws_region: “us-west-2” EOF ::: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy.yaml :::\nSend the request specifing the model:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “what is pi?” } ], “model”: “us.meta.llama3-3-70b-instruct-v1:0” }’ :::\nor\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “what is pi?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nNote the Kong AI Proxy plugin adds a new X-Kong-LLM-Model header with the model we consumer: bedrock/us.meta.llama3-3-70b-instruct-v1:0 or bedrock/us.amazon.nova-lite-v1:0\nStreaming Normally, a request is processed and completely buffered by the LLM before being sent back to Kong AI Gateway and then to the caller in a single large JSON block. This process can be time-consuming, depending on the request parameters, and the complexity of the request sent to the LLM model. To avoid making the user wait for their chat response with a loading animation, most models can stream each word (or sets of words and tokens) back to the client. This allows the chat response to be rendered in real time.\nThe config AI Proxy configuration section has a response_streaming parameter to define the response streaming. By default is set as allow but it can be set with deny or always.\nAs an example, if you send the same request with the stream parameter as true you should see a response like this:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “what is pi?” } ], “model”: “us.amazon.nova-lite-v1:0”, “stream”: true }’ :::\ndata: {\"object\":\"chat.completion.chunk\",\"choices\":[{\"logprobs\":null,\"delta\":{\"content\":\"\",\"role\":\"assistant\"},\"index\":0}],\"system_fingerprint\":null} data: {\"object\":\"chat.completion.chunk\",\"choices\":[{\"logprobs\":null,\"delta\":{\"content\":\"Pi\"},\"index\":0}],\"system_fingerprint\":null} data: {\"object\":\"chat.completion.chunk\",\"choices\":[{\"logprobs\":null,\"delta\":{\"content\":\" (π\"},\"index\":0}],\"system_fingerprint\":null} data: {\"object\":\"chat.completion.chunk\",\"choices\":[{\"logprobs\":null,\"delta\":{\"content\":\") is a mathematical\"},\"index\":0}],\"system_fingerprint\":null} data: {\"object\":\"chat.completion.chunk\",\"choices\":[{\"logprobs\":null,\"delta\":{\"content\":\" constant that\"},\"index\":0}],\"system_fingerprint\":null} ... data: {\"object\":\"chat.completion.chunk\",\"choices\":[{\"logprobs\":null,\"delta\":{\"content\":\"\"},\"index\":0}],\"system_fingerprint\":null} data: {\"choices\":[{\"logprobs\":null,\"finish_reason\":\"stop\",\"delta\":{},\"index\":0}],\"object\":\"chat.completion.chunk\"} data: [DONE] Extra Model Options The Kong AI Proxy provides other configuration options. For example:\nmax_tokens: defines the max_tokens, if using chat or completion models. By default, it is set as 256. Also, there are options to influence response generation with inference parameters:\ntemperature: it is a number between 0 and 5 and it defines the matching temperature, if using chat or completion models. top_p: a number between 0 and 1 defining the top-p probability mass, if supported. top_k: an integer between 0 and 500 defining the top-k most likely tokens, if supported. Kong-gratulations! have now reached the end of this module by caching API responses. You can now click Next to proceed with the next module.",
    "description": "The AI Proxy plugin is the fundamental AI Gateway component. It lets you transform and proxy requests to a number of AI providers and models. The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nThe following table describes which providers and requests the AI Proxy plugin supports:",
    "tags": [],
    "title": "AI Proxy plugin",
    "uri": "/16-ai-gateway/17-use-cases/150-ai-proxy/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.\nKong Gateway Plugin list Before enabling the Proxy Caching, let’s check the list of plugins Konnect provides. Inside the kong-aws Control Plane, click on Plugins menu option and + New plugin. You should the following page with all plugins available:\nEnabling a Kong Plugin on a Kong Service Create another declaration with plugins option. With this option you can enable and configure the plugin on your Kong Service.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 plugins: - name: proxy-cache instance_name: proxy-cache1 config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route EOF For the plugin configuration we used the following settings:\nstrategy with memory. The plugin will use the Runtime Instance’s memory to implement to cache. cache_ttl with 30, which means the plugin will clear all data that reached this time limit. All plugin configuration paramenters are described inside Kong Plugin Hub portal, in its specific documentation page.\nSubmit the new declaration deck gateway sync --konnect-token $PAT httpbin.yaml Expected Output\ncreating plugin proxy-cache for service httpbin-service Summary: Created: 1 Updated: 0 Deleted: 0 Consume the Service If you consume the service again, you’ll see some new headers describing the caching status:\ncurl -v $DATA_PLANE_LB/httpbin-route/get * Host a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com:80 was resolved. * IPv6: (none) * IPv4: 3.12.182.158, 18.216.117.235 * Trying 3.12.182.158:80... * Connected to a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com (3.12.182.158) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Content-Length: 443 \u003c Connection: keep-alive \u003c X-Cache-Key: 631505758a8c7ccfea4694d7b0164f6b4deaebc64c36c906813c79ba4ab906f3 \u003c X-Cache-Status: Miss \u003c Server: gunicorn \u003c Date: Tue, 27 May 2025 22:37:06 GMT \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Credentials: true \u003c X-Kong-Upstream-Latency: 2 \u003c X-Kong-Proxy-Latency: 1 \u003c Via: 1.1 kong/3.10.0.1-enterprise-edition \u003c X-Kong-Request-Id: eda0954efb9dc903147b4325b0a73563 \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"eda0954efb9dc903147b4325b0a73563\"},\"origin\":\"192.168.61.217\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com left intact Notice that, for the first request we get Miss for the X-Cache-Status header, meaning that the Runtime Instance didn’t have any data avaialble in the cache and had to connect to the Upstream Service, httpbin.org.\nIf we send a new request, the Runtime Instance has all it needs to satify the request, therefore the status is Hit. Note that the latency time has dropped considerably.\n# curl -v $DATA_PLANE_LB/httpbin-route/get * Host a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com:80 was resolved. * IPv6: (none) * IPv4: 18.216.117.235, 3.12.182.158 * Trying 18.216.117.235:80... * Connected to a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com (18.216.117.235) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Connection: keep-alive \u003c X-Cache-Key: 631505758a8c7ccfea4694d7b0164f6b4deaebc64c36c906813c79ba4ab906f3 \u003c Access-Control-Allow-Credentials: true \u003c age: 2 \u003c X-Cache-Status: Hit \u003c Access-Control-Allow-Origin: * \u003c Server: gunicorn \u003c Date: Tue, 27 May 2025 22:37:06 GMT \u003c Content-Length: 443 \u003c X-Kong-Upstream-Latency: 0 \u003c X-Kong-Proxy-Latency: 1 \u003c Via: 1.1 kong/3.10.0.1-enterprise-edition \u003c X-Kong-Request-Id: d148241a9c5e904a0b68235008315038 \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"eda0954efb9dc903147b4325b0a73563\"},\"origin\":\"192.168.61.217\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com left intact Enabling a Kong Plugin on a Kong Route Now, we are going to define a Rate Limiting policy for our Service. This time, you are going to enable the Rate Limiting plugin to the Kong Route, not to the Kong Gateway Service. In this sense, new Routes defined for the Service will not have the Rate Limiting plugin enabled, only the Proxy Caching.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF The configuration includes:\nminute as 3, which means the Route can be consumed only 3 times a given minute. Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml Consume the Service If you consume the service again, you’ll see, besides the caching related headers, new ones describing the status of current rate limiting policy:\ncurl -v $DATA_PLANE_LB/httpbin-route/get * Host a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com:80 was resolved. * IPv6: (none) * IPv4: 3.12.182.158, 18.216.117.235 * Trying 3.12.182.158:80... * Connected to a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com (3.12.182.158) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json \u003c Content-Length: 443 \u003c Connection: keep-alive \u003c RateLimit-Limit: 3 \u003c RateLimit-Reset: 51 \u003c X-RateLimit-Remaining-Minute: 2 \u003c X-RateLimit-Limit-Minute: 3 \u003c RateLimit-Remaining: 2 \u003c X-Cache-Key: 631505758a8c7ccfea4694d7b0164f6b4deaebc64c36c906813c79ba4ab906f3 \u003c X-Cache-Status: Miss \u003c Server: gunicorn \u003c Date: Tue, 27 May 2025 22:39:09 GMT \u003c Access-Control-Allow-Origin: * \u003c Access-Control-Allow-Credentials: true \u003c X-Kong-Upstream-Latency: 2 \u003c X-Kong-Proxy-Latency: 2 \u003c Via: 1.1 kong/3.10.0.1-enterprise-edition \u003c X-Kong-Request-Id: de0e817681d25556c8cfd01fcbaf1645 \u003c {\"args\":{},\"headers\":{\"Accept\":\"*/*\",\"Connection\":\"keep-alive\",\"Host\":\"httpbin.kong.svc.cluster.local:8000\",\"User-Agent\":\"curl/8.7.1\",\"X-Forwarded-Host\":\"a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com\",\"X-Forwarded-Path\":\"/httpbin-route/get\",\"X-Forwarded-Prefix\":\"/httpbin-route\",\"X-Kong-Request-Id\":\"de0e817681d25556c8cfd01fcbaf1645\"},\"origin\":\"192.168.61.217\",\"url\":\"http://httpbin.kong.svc.cluster.local:8000/get\"} * Connection #0 to host a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com left intact If you keep sending new requests to the Runtime Instance, eventually, you’ll get a 429 error code, meaning you have reached the consumption rate limiting policy for this Route.\ncurl -v $DATA_PLANE_LB/httpbin-route/get * Host a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com:80 was resolved. * IPv6: (none) * IPv4: 3.12.182.158, 18.216.117.235 * Trying 3.12.182.158:80... * Connected to a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com (3.12.182.158) port 80 \u003e GET /httpbin-route/get HTTP/1.1 \u003e Host: a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com \u003e User-Agent: curl/8.7.1 \u003e Accept: */* \u003e * Request completely sent off \u003c HTTP/1.1 429 Too Many Requests \u003c Date: Tue, 27 May 2025 22:39:15 GMT \u003c Content-Type: application/json; charset=utf-8 \u003c Connection: keep-alive \u003c RateLimit-Limit: 3 \u003c Retry-After: 45 \u003c RateLimit-Reset: 45 \u003c RateLimit-Remaining: 0 \u003c X-RateLimit-Limit-Minute: 3 \u003c X-RateLimit-Remaining-Minute: 0 \u003c Content-Length: 92 \u003c X-Kong-Response-Latency: 0 \u003c Server: kong/3.10.0.1-enterprise-edition \u003c X-Kong-Request-Id: 78a9a40f86bf6f7856ea932ce3bf2029 \u003c { \"message\":\"API rate limit exceeded\", \"request_id\":\"78a9a40f86bf6f7856ea932ce3bf2029\" * Connection #0 to host a06491acb99f64d0481263f3536909da-1064984904.us-east-2.elb.amazonaws.com left intact } Enabling a Kong Plugin globally Besides scoping a plugin to a Kong Service or Route, we can apply it globally also. When we do it so, all Services ans Routes will enforce the police described by the plugin.\nFor example, let’s apply the Proxy Caching plugin globally.\ncat \u003e httpbin.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-workshop _info: select_tags: - httpbin-service-route plugins: - name: proxy-cache config: strategy: memory cache_ttl: 30 services: - name: httpbin-service tags: - httpbin-service-route host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: httpbin-route tags: - httpbin-service-route paths: - /httpbin-route plugins: - name: rate-limiting instance_name: rate-limiting1 config: minute: 3 EOF Submit the declaration deck gateway sync --konnect-token $PAT httpbin.yaml After testing the configuration reset the Control Plane:\ndeck gateway reset --konnect-control-plane-name kong-aws --konnect-token $PAT -f Kong-gratulations! have now reached the end of this module by caching API responses. You can now click Next to proceed with the next module.",
    "description": "Proxy Caching provides a reverse proxy cache implementation for Kong. It caches response entities based on configurable response code and content type, as well as request method. It can cache per-Consumer or per-API. Cache entities are stored for a configurable period of time, after which subsequent requests to the same resource will re-fetch and re-store the resource. Cache entities can also be forcefully purged via the Admin API prior to their expiration time.",
    "tags": [],
    "title": "Proxy Caching",
    "uri": "/12-api-gateway/15-use-cases/150-proxy-caching/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching API Key with Kong Consumers Rate Limiting using Redis Request Transformer Request Callout OpenID Connect with Keycloak These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nProxy caching API Key with Kong Consumers Rate Limiting using Redis Request Transformer Request Callout OpenID Connect with Keycloak These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/12-api-gateway/15-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.\nA Kong Consumer represents a consumer (user or application) of a Service. A Kong Consumer is tightly coupled to an Authentication mechanism the Kong Gateway provides.\nPlease, check the Key-Auth plugin plugin and Kong Consumer documentation pages to learn more about them.\nEnable the Key Authentication Plugin on the Kong Route :::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e key-auth.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: httpbin-route paths: /key-auth-route plugins: name: key-auth instance_name: key-auth1 EOF ::: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway sync –konnect-token $PAT key-auth.yaml :::\nConsume the Route Now, if you try the Route, you’ll get a specific 401 error code meaning that, since you don’t have any API Key injected in your request, you are not allowd to consume it.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i $DATA_PLANE_LB/key-auth-route/get :::\nHTTP/1.1 401 Unauthorized Date: Wed, 28 May 2025 12:05:25 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 96 X-Kong-Response-Latency: 0 Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 3bdc0f233664705be8414e1b29ace607 { \"message\":\"No API key found in request\", \"request_id\":\"3bdc0f233664705be8414e1b29ace607\" } Create a Kong Consumer In order to consume the Route we need to create a Kong Consumer. Here’s its declaration:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e key-auth.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: httpbin-route paths: /key-auth-route plugins: name: key-auth instance_name: key-auth1 consumers: keyauth_credentials: key: “123456” username: consumer1 EOF ::: Submit the declaration :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway sync –konnect-token $PAT key-auth.yaml :::\nConsume the Route with the API Key Now, you need to inject the Key you’ve just created, as a header, in your requests. Using HTTPie, you can do it easily like this:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl –head $DATA_PLANE_LB/key-auth-route/get -H ‘apikey:123456’ :::\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 617 Connection: keep-alive Server: gunicorn Date: Wed, 28 May 2025 12:07:57 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 1 X-Kong-Proxy-Latency: 5 Via: 1.1 kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 4b58c81d7e6d937ec596a22f2635834e Of course, if you inject a wrong key, you get a specific error like this:\n# curl --head $DATA_PLANE_LB/key-auth-route/get -H 'apikey:12' HTTP/1.1 401 Unauthorized Date: Wed, 28 May 2025 12:08:38 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 81 X-Kong-Response-Latency: 1 Server: kong/3.10.0.2-enterprise-edition X-Kong-Request-Id: 325bee25eda87c5358c3e06334f07d74 NOTE\nThe header has to have the API Key name, which is, in our case, apikey. That was the default name provided by Konnect when you enabled the Key Authentication on the Kong Route. You can change the plugin configuration, if you will. Kong Consumer Policies With the API Key policy in place, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIt’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 5 rpm consumer2: apikey = 987654 rate limiting policy = 8 rpm Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer.\nFor this section we’re implementing a Rate Limiting policy. Keep in mind that a Consumer might have other plugins also enabled such as Request Transformer, TCP Log, etc.\nNew Consumer Create the second consumer2, just like you did with the first one, with the 987654 key.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e key-auth.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: httpbin-route paths: /key-auth-route plugins: name: key-auth instance_name: key-auth1 consumers: keyauth_credentials: key: “123456” username: consumer1 keyauth_credentials: key: “987654” username: consumer2 EOF ::: Submit the declaration :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway sync –konnect-token $PAT key-auth.yaml :::\nIf you will, you can inject both keys to your requests.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl –head $DATA_PLANE_LB/key-auth-route/get -H ‘apikey:123456’ :::\nor\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl –head $DATA_PLANE_LB/key-auth-route/get -H ‘apikey:987654’ :::\nConsumers’ Policy Now let’s enhance the plugins declaration enabling the Rate Limiting plugin to each one of our consumers.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e key-auth.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: httpbin-route paths: /key-auth-route plugins: name: key-auth instance_name: key-auth1 consumers: keyauth_credentials: key: “123456” username: consumer1 plugins: name: rate-limiting instance_name: rate-limiting1 config: minute: 5 keyauth_credentials: key: “987654” username: consumer2 plugins: name: rate-limiting instance_name: rate-limiting2 config: minute: 8 EOF ::: Submit the declaration :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway sync –konnect-token $PAT key-auth.yaml :::\nConsumer the Route using different API Keys. First of all let’s consume the Route with the Consumer1’s API Key:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl –head $DATA_PLANE_LB/key-auth-route/get -H ‘apikey:123456’ :::\nExpected Output\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 601 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 4 RateLimit-Reset: 35 RateLimit-Remaining: 4 RateLimit-Limit: 5 Server: gunicorn Date: Fri, 09 May 2025 13:04:25 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 2 X-Kong-Proxy-Latency: 1 Via: 1.1 kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 7b9f662e2e0d2f562b12535227ff024b Now, let’s consume it with the Consumer2’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl –head $DATA_PLANE_LB/key-auth-route/get -H ‘apikey:123456’ :::\nExpected Output\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 617 Connection: keep-alive RateLimit-Limit: 5 RateLimit-Reset: 50 RateLimit-Remaining: 4 X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 4 Server: gunicorn Date: Wed, 28 May 2025 12:15:10 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 2 X-Kong-Proxy-Latency: 1 Via: 1.1 kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: d927f9253a57be7775bdd67e0e8f0328 If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl –head $DATA_PLANE_LB/key-auth-route/get -H ‘apikey:123456’ :::\nExpected Output\nHTTP/1.1 429 Too Many Requests Date: Wed, 28 May 2025 12:15:28 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive RateLimit-Limit: 5 Retry-After: 32 RateLimit-Reset: 32 RateLimit-Remaining: 0 X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 0 Content-Length: 92 X-Kong-Response-Latency: 0 Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: cf9e1a060a3d68737a805a732922ac19 However, the second API Key is still allowed to consume the Kong Route:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl –head $DATA_PLANE_LB/key-auth-route/get -H ‘apikey:987654’ :::\nExpected Output\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 617 Connection: keep-alive RateLimit-Limit: 8 RateLimit-Reset: 26 RateLimit-Remaining: 6 X-RateLimit-Limit-Minute: 8 X-RateLimit-Remaining-Minute: 6 Server: gunicorn Date: Wed, 28 May 2025 12:15:34 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 2 X-Kong-Proxy-Latency: 1 Via: 1.1 kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: aaa3c84c4226d21e1c178e598cfb791e Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.\nOptional Reading Applying Kong Plugins on Services, Routes or Globally helps us to implement an extensive list of policies in the API Gateway layer. However, so far, we are not controlling who is sending the requests to the Data Plane. That is, anyone who has the Runtime Instance ELB address is capable to send requests to it and consumer the Services.\nAPI Gateway Authentication is an important way to control the data that is allowed to be transmitted using your APIs. Basically, it checks that a particular consumer has permission to access the API, using a predefined set of credentials.\nKong Gateway has a library of plugins that provide simple ways to implement the best known and most widely used methods of API gateway authentication. Here are some of the commonly used ones:\nBasic Authentication Key Authentication OAuth 2.0 Authentication LDAP Authentication OpenID Connect Kong Plugin Hub provides documentation about all Authentication based plugins. Refer to the following link to read more about API Gateway Authentication",
    "description": "To get started with API Authentication, let’s implement a basic Key Authentication mechanism. API Keys are one of the foundamental security mechanisms provided by Konnect. In order to consume an API, the consumer should inject a previously created API Key in the header of the request. The API consumption is allowed if the Gateway recognizes the API Key. Consumers add their API key either in a query string parameter, a header, or a request body to authenticate their requests and consume the application.",
    "tags": [],
    "title": "API Key Authentication",
    "uri": "/12-api-gateway/15-use-cases/151-key-authentication/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over how an LLM service controlled by Amazon Bedrock. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.\nYou can now click Next to proceed further.",
    "description": "For Prompt Engineering use cases, Kong AI Gateway provides:\nAI Prompt Decorator plugin: it injects messages at the start or end of a caller’s chat history. AI Prompt Guard plugin: it lets you configure a series of PCRE-compatible regular expressions to allow and block specific prompts, words, phrases, or otherwise and have more control over how an LLM service controlled by Amazon Bedrock. AI Prompt Template plugin: it’s responsible for pre-configuring AI prompts to users All plugins extend the functionality of the AI Proxy plugin, and requires AI Proxy to be configured first.",
    "tags": [],
    "title": "Prompt Engineering",
    "uri": "/16-ai-gateway/17-use-cases/151-prompt-engineering/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-aws _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration deck gateway sync --konnect-token $PAT response-transformer.yaml Verify Test to make sure Kong transforms the request to the echo server and httpbin server.\ncurl --head $DATA_PLANE_LB/response-transformer-route/get HTTP/1.1 200 OK Content-Type: application/json Content-Length: 469 Connection: keep-alive Server: gunicorn Date: Wed, 28 May 2025 12:24:14 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true demo: injected-by-kong X-Kong-Upstream-Latency: 2 X-Kong-Proxy-Latency: 1 Via: 1.1 kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: a08fac3ca8cc994a3d90bd70ece7745a Expected Results Notice that demo: injected-by-kong is injected in the header.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\ndeck gateway reset --konnect-control-plane-name kong-aws --konnect-token $PAT -f In real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module by configuring the Kong Route to include demo: injected-by-kong before responding to the client. You can now click Next to proceed with the next module.",
    "description": "The Response Transformer plugin modifies the upstream response (e.g. response from the server) before returning it to the client.\nIn this section, you will configure the Response Transformer plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\ncat \u003e response-transformer.yaml \u003c\u003c 'EOF' _format_version: \"3.0\" _konnect: control_plane_name: kong-aws _info: select_tags: - httpbin-service-route services: - name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: - name: response-transformer-route paths: - /response-transformer-route plugins: - name: response-transformer instance_name: response-transformer1 config: add: headers: - demo:injected-by-kong EOF Submit the declaration deck gateway sync --konnect-token $PAT response-transformer.yaml",
    "tags": [],
    "title": "Response Transformer Plugin",
    "uri": "/12-api-gateway/15-use-cases/153-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e response-transformer.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: response-transformer-route paths: /response-transformer-route plugins: name: request-callout instance_name: request-callout1 config: callouts: name: wikipedia request: url: en.wikipedia.org/w/api.php method: GET forward: false query: - srsearch:theorem - action:query - list:search - format:json response: body: decode: true upstream: by_lua: kong.response.exit(200, { uuid = kong.ctx.shared.callouts.c1.response.body.uuid, origin = kong.ctx.shared.callouts.c2.response.body.url}) EOF ::: Submit the declaration :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway sync –konnect-token $PAT response-transformer.yaml :::\nVerify Test to make sure Kong transforms the request to the echo server and httpbin server.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl –head $DATA_PLANE_LB/response-transformer-route/get :::\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 469 Connection: keep-alive Server: gunicorn Date: Wed, 28 May 2025 12:24:14 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true demo: injected-by-kong X-Kong-Upstream-Latency: 2 X-Kong-Proxy-Latency: 1 Via: 1.1 kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: a08fac3ca8cc994a3d90bd70ece7745a Expected Results Notice that demo: injected-by-kong is injected in the header.\nCleanup Reset the Control Plane to ensure that the plugins do not interfere with any other modules in the workshop for demo purposes and each workshop module code continues to function independently.\n:::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f :::\nIn real world scenario, you can enable as many plugins as you like depending on your use cases.\nKong-gratulations! have now reached the end of this module by configuring the Kong Route to include demo: injected-by-kong before responding to the client. You can now click Next to proceed with the next module.",
    "description": "The Request Callout plugin allows you to insert arbitrary API calls before proxying a request to the upstream service.\nIn this section, you will configure the Request Callout plugin on the Kong Route. Specifically, you will configure Kong Konnect to add a new header “demo: injected-by-kong” before responding to the client.\nCreate the Response Transformer Plugin Take the plugins declaration and enable the Response Transformer plugin to the Route.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e response-transformer.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:",
    "tags": [],
    "title": "Request Callout Plugin",
    "uri": "/12-api-gateway/15-use-cases/154-request-callout/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.\nThe AI Request Transformer plugin runs before all of the AI Prompt plugins and the AI Proxy plugin, allowing it to also introspect LLM requests against the same, or a different, LLM. On the other hand, the AI Response Transformer plugin runs after the AI Proxy plugin, and after proxying to the Upstream Service, allowing it to also introspect LLM responses against the same, or a different, LLM service.\nThe diagram shows the journey of a consumer’s request through Kong Gateway to the backend service, where it is transformed by both an AI LLM service and Kong’s AI Request Transformer and the AI Response Transformer plugins.\nFor each plugin the configuration and usage processes are:\nThe Kong Gateway admin sets up an llm: configuration block, following the same configuration format as the AI Proxy plugin, and the same driver capabilities. The Kong Gateway admin sets up a prompt for the request introspection. The prompt becomes the system message in the LLM chat request, and prepares the LLM with transformation instructions for the incoming user request body (for the AI Request Transformer plugin) and for the returning upstream response body (for the AI Response Transformer plugin) The user makes an HTTP(S) call. Before proxying the user’s request to the backend, Kong Gateway sets the entire request body as the user message in the LLM chat request, and then sends it to the configured LLM service. After receiving the response from the backend, Kong Gateway sets the entire response body as the user message in the LLM chat request, then sends it to the configured LLM service. The LLM service returns a response assistant message, which is subsequently set as the upstream request body. The following example is going to apply the plugins to transform both request and reponse when consuming the httpbin Upstream Service.\nNow, configure both plugins. Keep in mind that the plugins are totally independent from each other so, the configuration depends on your use case.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-request-response-tranformer.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nbedrock services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: httpbin-route paths: /httpbin-route plugins: name: ai-request-transformer instance_name: ai-request-transformer enabled: true config: prompt: In my JSON message, anywhere there is a JSON tag for a “city”, also add a “country” tag with the name of the country in which the city resides. Return me only the JSON message, no extra text.\" llm: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: llm/v1/chat model: name: “us.meta.llama3-3-70b-instruct-v1:0” provider: bedrock options: bedrock: aws_region: us-west-2 name: ai-response-transformer instance_name: ai-response-transformer enabled: true config: prompt: For any city name, add its current temperature, in brackets next to it. Reply with the JSON result only. llm: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: llm/v1/chat model: name: “us.meta.llama3-3-70b-instruct-v1:0” provider: bedrock options: bedrock: aws_region: us-west-2 EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-request-response-tranformer.yaml :::\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/httpbin-route/post –header ‘Content-Type: application/json’ –data ‘{ “user”: { “name”: “Kong User”, “city”: “Tokyo” } }’ | jq :::\nExpected output { \"user\": { \"name\": \"Kong User\", \"city\": \"Tokyo [12°C]\", \"country\": \"Japan\" } } Kong-gratulations! have now reached the end of this module by using Kong Gateway to invoke a AWS Lambda function. You can now click Next to proceed with the next chapter.",
    "description": "The AI Request Transformer and AI Response Transformer plugins integrate with the LLM on Amazon Bedrock, enabling introspection and transformation of the request’s body before proxying it to the Upstream Service and prior to forwarding the response to the client.\nThe plugins support llm/v1/chat style requests for all of the same providers as the AI Proxy plugin. It also use all of the same configuration and tuning parameters as the AI Proxy plugin, under the config.llm block.",
    "tags": [],
    "title": "AI Request and Response Transfomer plugins",
    "uri": "/16-ai-gateway/17-use-cases/155-request-response-transformer/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "Semantic caching enhances data retrieval efficiency by focusing on the meaning or context of queries rather than just exact matches. It stores responses based on the underlying intent and semantic similarities between different queries and can then retrieve those cached queries when a similar request is made.\nWhen a new request is made, the system can retrieve and reuse previously cached responses if they are contextually relevant, even if the phrasing is different. This method reduces redundant processing, speeds up response times, and ensures that answers are more relevant to the user’s intent, ultimately improving overall system performance and user experience.\nFor example, if a user asks, “how to integrate our API with a mobile app” and later asks, “what are the steps for connecting our API to a smartphone application?”, the system understands that both questions are asking for the same information. It can then retrieve and reuse previously cached responses, even if the wording is different. This approach reduces processing time and speeds up responses.\nThe AI Semantic Cache plugin may not be ideal for you if:\nIf you have limited hardware or budget. Storing semantic vectors and running similarity searches require a lot of storage and computing power, which could be an issue. If your data doesn’t rely on semantics, or exact matches work fine, semantic caching may offer little benefit. Traditional or keyword-based caching might be more efficient. How it works The diagram below illustrates the semantic caching mechanism implemented by the AI Semantic Cache plugin.\nThe process involves three parts: request handling, embedding generation, and response caching.\nFirst, a user starts a chat request with the LLM. The AI Semantic Cache plugin queries the vector database to see if there are any semantically similar requests that have already been cached. If there is a match, the vector database returns the cached response to the user. If there isn’t a match, the AI Semantic Cache plugin prompts the embeddings LLM to generate an embedding for the response. The AI Semantic Cache plugin uses a vector database and cache to store responses to requests. The plugin can then retrieve a cached response if a new request matches the semantics of a previous request, or it can tell the vector database to store a new response if there are no matches. With the AI Semantic Cache plugin, you can configure a cache of your choice to store the responses from the LLM. Currently, the plugin supports Redis as a cache.\nRedis as a Vector database We are going to configure the AI Semantic Cache to consume the Redis deployment available in the EKS Cluster. Redis, this time, will play the Vector database role.\nApply the Semantic Cache plugin :::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-semantic-cache.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:\nsemantic-cache _info: select_tags: bedrock _konnect: control_plane_name: kong-aws services: name: service1 host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy instance_name: ai-proxy-bedrock-route enabled: true config: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: llm/v1/chat model: provider: bedrock options: bedrock: aws_region: us-west-2 name: ai-semantic-cache instance_name: ai-semantic-cache-bedrock enabled: true config: embeddings: auth: param_name: “allow_override” param_value: “false” param_location: “body” model: provider: bedrock name: “amazon.titan-embed-text-v2:0” options: bedrock: aws_region: us-west-2 vectordb: dimensions: 1024 distance_metric: cosine strategy: redis threshold: 0.2 redis: host: “redis-stack.redis.svc.cluster.local” port: 6379 EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-semantic-cache.yaml :::\nCheck Redis Before sending request, you can scan the Redis database:\n:::code{showCopyAction=true showLineNumbers=false language=shell} kubectl exec -it $(kubectl get pod -n redis -o json | jq -r ‘.items[].metadata.name’) -n redis – redis-cli –scan :::\n1st Request Since we don’t have any cached data, the first request is going to return “Miss”:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is Jimi Hendrix?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nExpected response HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-Cache-Status: Miss Date: Fri, 18 Apr 2025 14:41:32 GMT Content-Length: 2703 x-amzn-RequestId: 08a965e0-60b4-457f-aacd-273cb6940988 X-Kong-LLM-Model: bedrock/us.amazon.nova-lite-v1:0 X-Kong-Upstream-Latency: 3440 X-Kong-Proxy-Latency: 97 Via: 1.1 kong/3.10.0.1-enterprise-edition Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 80eeaf284d891a588e55fb22b95f814b {\"usage\":{\"completion_tokens\":557,\"prompt_tokens\":5,\"total_tokens\":562},\"choices\":[{\"finish_reason\":\"stop\",\"message\":{\"content\":\"Jimi Hendrix was an American rock guitarist, singer, and songwriter widely regarded as one of the greatest and most influential guitarists in the history of popular music. Born on November 27, 1942, in Seattle, Washington, he became a pivotal figure in the 1960s psychedelic and hard rock movements.\\n\\n### Early Life and Career\\nHendrix's interest in music began at a young age. He started playing guitar at 15 and later served in the U.S. Army, where he was a member of the Army band. After leaving the Army, he moved to Nashville, Tennessee, and then to New York City, where he played in various R\u0026B bands.\\n\\n### Breakthrough\\nHendrix moved to London in 1966, where he formed the Jimi Hendrix Experience with bassist Noel Redding and drummer Mitch Mitchell. The band quickly gained popularity in Europe, and their debut album, \\\"Are You Experienced,\\\" released in 1967, brought them international acclaim.\\n\\n### Musical Innovation\\nHendrix was known for his innovative and highly influential guitar playing. He used feedback, distortion, and other effects to create a new sound that pushed the boundaries of rock music. His performances were characterized by his virtuosic technique, including playing the guitar behind his back, between his legs, and with his teeth.\\n\\n### Notable Works\\n- **\\\"Are You Experienced\\\" (1967)**: The band's debut album, featuring hits like \\\"Purple Haze\\\" and \\\"Hey Joe.\\\"\\n- **\\\"Axis: Bold as Love\\\" (1967)**: Known for its complex compositions and innovative use of studio effects.\\n- **\\\"Electric Ladyland\\\" (1968)**: A double album that showcased the band's experimental approach and included the iconic track \\\"Voodoo Child (Slight Return).\\\"\\n- **Woodstock (1969)**: His performance at the Woodstock Festival, particularly his rendition of \\\"The Star-Spangled Banner,\\\" is considered one of the most legendary moments in rock history.\\n\\n### Legacy\\nHendrix's influence extends far beyond his lifetime. He has inspired countless musicians across various genres, and his innovative approach to the guitar has left a lasting impact on rock music. He died on September 18, 1970, at the age of 27, but his music continues to be celebrated and studied.\\n\\n### Honors and Recognition\\nHendrix has been inducted into multiple halls of fame, including the Rock and Roll Hall of Fame and the UK Music Hall of Fame. He has received numerous awards and accolades, including Grammy Awards, and is often ranked among the greatest guitarists of all time by various music publications.\",\"role\":\"assistant\"},\"index\":0}],\"object\":\"chat.completion\"} Check Redis again The Redis database has an entry now: :::code{showCopyAction=true showLineNumbers=false language=shell} kubectl exec -it $(kubectl get pod -n redis -o json | jq -r ‘.items[].metadata.name’) -n redis – redis-cli –scan :::\nExpected response \"kong_semantic_cache:9eb0bd74-b166-46a7-b478-d46910134e2b:bedrock-us.amazon.nova-lite-v1:0:6e373ab243f6868432eae1f532c4cb11849d502b60a9a6644a6f24d7e89cc4bf\" 2nd Request The Semantic Cache plugin will use the cached data for similar requests:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Tell me more about Jimi Hendrix” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nExpected response HTTP/1.1 200 OK Date: Fri, 18 Apr 2025 14:44:21 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-Cache-Status: Hit Age: 3 X-Cache-Key: kong_semantic_cache:9eb0bd74-b166-46a7-b478-d46910134e2b:bedrock-us.amazon.nova-lite-v1:0:7bb5349c15e9068e3dfca4f83bd2f40fbdf42aa892b93622f8759f58273db18d X-Cache-Ttl: 297 Content-Length: 3653 X-Kong-Response-Latency: 73 Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: deab9b016f23907e1b737a0d8fea6a5d {\"usage\":{\"completion_tokens\":731,\"prompt_tokens\":6,\"total_tokens\":737},\"choices\":[{\"finish_reason\":\"stop\",\"message\":{\"content\":\"Jimi Hendrix was an iconic American rock guitarist, singer, and songwriter, widely regarded as one of the greatest musicians in the history of rock music. Born on November 27, 1942, in Seattle, Washington, Hendrix became a pivotal figure in the 1960s counterculture and is celebrated for his groundbreaking guitar techniques and innovative approach to music.\\n\\n### Early Life and Career\\nHendrix's early years were marked by a deep interest in music, which he pursued despite facing racial discrimination. He began playing in local bands in the mid-1960s before moving to New York City, where he played in various R\u0026B and soul bands. His big break came when he moved to London in 1966, where he formed the Jimi Hendrix Experience with bassist Noel Redding and drummer Mitch Mitchell.\\n\\n### The Jimi Hendrix Experience\\nThe Jimi Hendrix Experience quickly gained a reputation for their electrifying live performances. Their debut album, \\\"Are You Experienced,\\\" released in 1967, showcased Hendrix's innovative guitar playing, characterized by his use of feedback, distortion, and other effects to create a unique sound. Tracks like \\\"Purple Haze\\\" and \\\"Hey Joe\\\" became instant classics.\\n\\n### Iconic Performances\\nHendrix's performances at major music festivals, such as the Monterey Pop Festival (1967) and Woodstock (1969), are legendary. His rendition of \\\"The Star-Spangled Banner\\\" at Monterey, where he played the song with feedback and distortion, remains one of the most iconic moments in rock history. At Woodstock, his performance was a highlight, featuring songs like \\\"Voodoo Child (Slight Return)\\\" and \\\"Purple Haze.\\\"\\n\\n### Innovations and Techniques\\nHendrix revolutionized the role of the electric guitar in rock music. He was a master of using the guitar as a percussive and melodic instrument, often incorporating elements of blues, jazz, and psychedelic rock. His techniques included playing with his teeth and behind his back, using the guitar as a percussive instrument, and employing innovative studio effects.\\n\\n### Studio Albums\\nHendrix released several influential albums during his career, including:\\n- **\\\"Are You Experienced\\\" (1967)**: The debut album that introduced his innovative sound.\\n- **\\\"Axis: Bold as Love\\\" (1967)**: Featuring hits like \\\"Castles Made of Sand\\\" and \\\"Little Wing.\\\"\\n- **\\\"Electric Ladyland\\\" (1968)**: A double album that showcased the band's studio experimentation and live performances.\\n- **\\\"Band of Gypsys\\\" (1969)**: Recorded with Billy Cox and Buddy Miles, it featured a more raw and powerful sound.\\n\\n### Legacy\\nHendrix's influence extends far beyond his lifetime. His innovative approach to guitar playing has inspired countless musicians across genres. He was inducted into the Rock and Roll Hall of Fame in 1990 and the UK Music Hall of Fame in 2005. His life and music continue to be celebrated, with numerous documentaries, biographies, and compilations dedicated to his work.\\n\\n### Personal Life and Death\\nHendrix's personal life was marked by struggles with addiction and mental health issues. He died on September 18, 1970, in London, at the age of 27. His death was ruled an accidental overdose, but it remains a subject of much speculation and debate.\\n\\nJimi Hendrix's legacy as a musical innovator and cultural icon endures, and his contributions to rock music continue to be celebrated and studied.\",\"role\":\"assistant\"},\"index\":0}],\"object\":\"chat.completion\",\"id\":\"7bb5349c15e9068e3dfca4f83bd2f40fbdf42aa892b93622f8759f58273db18d\"} 3rd Request As expected, for a non-related request, the AI Gateway will hit the LLM to satisfy the query:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who was Joseph Conrad?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nExpected response HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-Cache-Status: Miss Date: Fri, 18 Apr 2025 14:45:30 GMT Content-Length: 1465 x-amzn-RequestId: e23f122b-b4d7-4164-a417-1d9f20e82289 X-Kong-LLM-Model: bedrock/us.amazon.nova-lite-v1:0 X-Kong-Upstream-Latency: 1647 X-Kong-Proxy-Latency: 143 Via: 1.1 kong/3.10.0.1-enterprise-edition Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 6025a75733a29f47b2c7fd56c0c4771f {\"usage\":{\"completion_tokens\":296,\"prompt_tokens\":5,\"total_tokens\":301},\"choices\":[{\"finish_reason\":\"stop\",\"message\":{\"content\":\"Joseph Conrad (born Józef Teodor Konrad Korzeniowski) was a renowned Polish-British writer born on December 3, 1857, in Berdichev, then part of the Russian Empire (now Ukraine), and died on August 3, 1924, in Bishopsbourne, England. \\n\\nConrad is best known for his novels and short stories that explore themes of imperialism, existentialism, and the complexities of the human psyche. His works often feature seafaring settings and characters, reflecting his own experiences as a sailor. Some of his most famous works include:\\n\\n1. **\\\"Heart of Darkness\\\"** (1899) - A novella that critiques the brutal realities of colonialism in the Congo Free State.\\n2. **\\\"Lord Jim\\\"** (1900) - A novel that delves into themes of honor, failure, and redemption.\\n3. **\\\"Nostromo\\\"** (1904) - A novel set in the fictional South American country of Costaguana, exploring themes of imperialism and the corrupting influence of power.\\n4. **\\\"The Secret Agent\\\"** (1907) - A novel that examines the nature of terrorism and the psychology of the individual.\\n\\nConrad's writing is characterized by its complex narrative structures, rich symbolism, and deep psychological insight. He is considered one of the greatest novelists in the English language and a precursor to modernist literature.\",\"role\":\"assistant\"},\"index\":0}],\"object\":\"chat.completion\"} Check Redis again Redis database has two entries now:\n:::code{showCopyAction=true showLineNumbers=false language=shell} kubectl exec -it $(kubectl get pod -n redis -o json | jq -r ‘.items[].metadata.name’) -n redis – redis-cli –scan :::\nExpected response \"kong_semantic_cache:2398b6a1-85e4-4330-af08-4628f51254e7:bedrock-us.amazon.nova-lite-v1:0:6e373ab243f6868432eae1f532c4cb11849d502b60a9a6644a6f24d7e89cc4bf\" \"kong_semantic_cache:2398b6a1-85e4-4330-af08-4628f51254e7:bedrock-us.amazon.nova-lite-v1:0:60afd78b21ec82cf4d2264efd0b6faf081edd85c287f272e41508e5ddcd7dc50\" Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.",
    "description": "Semantic caching enhances data retrieval efficiency by focusing on the meaning or context of queries rather than just exact matches. It stores responses based on the underlying intent and semantic similarities between different queries and can then retrieve those cached queries when a similar request is made.\nWhen a new request is made, the system can retrieve and reuse previously cached responses if they are contextually relevant, even if the phrasing is different. This method reduces redundant processing, speeds up response times, and ensures that answers are more relevant to the user’s intent, ultimately improving overall system performance and user experience.",
    "tags": [],
    "title": "AI Semantic Cache plugin",
    "uri": "/16-ai-gateway/17-use-cases/156-semantic-cache/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "Kong can rate-limit your traffic without any external dependency. In such a case, Kong stores the request counters in-memory and each Kong node applies the rate-limiting policy independently. There is no synchronization of information being done in this case. But if Redis is available in your cluster, Kong can take advantage of it and synchronize the rate-limit information across multiple Kong nodes and enforce a slightly different rate-limiting policy.\nThis section walks through the steps of using Redis for rate-limiting in a multi-node Kong deployment.\nHigh Level Tasks You will complete the following:\nSet up rate-limiting plugin Scale Kong for Kubernetes to multiple pods Verify rate-limiting across cluster You can now click Next to proceed further.",
    "description": "Kong can rate-limit your traffic without any external dependency. In such a case, Kong stores the request counters in-memory and each Kong node applies the rate-limiting policy independently. There is no synchronization of information being done in this case. But if Redis is available in your cluster, Kong can take advantage of it and synchronize the rate-limit information across multiple Kong nodes and enforce a slightly different rate-limiting policy.\nThis section walks through the steps of using Redis for rate-limiting in a multi-node Kong deployment.",
    "tags": [],
    "title": "Rate Limiting Using Redis",
    "uri": "/12-api-gateway/15-use-cases/156-rate-limiting-using-redis/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases",
    "content": "OpenID Connect plugin allows the integration with a 3rd party identity provider (IdP) in a standardized way. This plugin can be used to implement Kong as a (proxying) OAuth 2.0 resource server (RS) and/or as an OpenID Connect relying party (RP) between the client, and the upstream service.\nIn this module, we will configure this plugin to use Amazon Cognito . A detailed integration guide is available here for future reading.\nCreating AWS Cognito Run the following command to create the AWS Cognito Resources using a CloudFormation templates\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl ‘:assetUrl{path=\"/code/cognito.yaml\"}’ –output cognito.yaml echo “export RANDOM_IDENTIFIER=$(uuidgen -r)” » ~/.bashrc bash aws cloudformation deploy –template-file cognito.yaml –stack-name cognito-$RANDOM_IDENTIFIER –parameter-overrides ClientName=$RANDOM_IDENTIFIER-client Domain=$RANDOM_IDENTIFIER PoolName=$RANDOM_IDENTIFIER-pool CallBackUrl=https://$DATA_PLANE_LB/oidc-route/get echo “export COGNITO_CLIENT_ID=$(aws cloudformation describe-stack-resources –stack-name cognito-$RANDOM_IDENTIFIER | jq -r ‘.StackResources[] | select(.ResourceType==“AWS::Cognito::UserPoolClient”) | .PhysicalResourceId’)” » ~/.bashrc bash echo “export COGNITO_POOL_ID=$(aws cloudformation describe-stack-resources –stack-name cognito-$RANDOM_IDENTIFIER | jq -r ‘.StackResources[] | select(.ResourceType==“AWS::Cognito::UserPool”) | .PhysicalResourceId’)” » ~/.bashrc bash echo “export ISSUER=https://cognito-idp.$AWS_REGION.amazonaws.com/$COGNITO_POOL_ID/.well-known/openid-configuration” » ~/.bashrc bash :::\nFetch the client secret\n:::code{showCopyAction=true showLineNumbers=false language=shell} echo “export CLIENT_SECRET=$(aws cognito-idp describe-user-pool-client –user-pool-id $COGNITO_POOL_ID –client-id $COGNITO_CLIENT_ID –query ‘UserPoolClient.ClientSecret’)” » ~/.bashrc bash :::\nInstalling OpenID Connect Plugin :::code{showCopyAction=true showLineNumbers=false language=shell} echo “_format_version: \"3.0\" _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: oidc-route paths: /oidc-route plugins: name: openid-connect instance_name: openid-connect1 config: auth_methods: authorization_code redirect_uri: https://$DATA_PLANE_LB/oidc-route/get client_id: $COGNITO_CLIENT_ID client_secret: $CLIENT_SECRET issuer: $ISSUER” \u003e ./oidc.yaml ::: Submit the declaration :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway sync –konnect-token $PAT oidc.yaml :::\nCheck your Client Id, Secret and Issuer\n:::code{showCopyAction=true showLineNumbers=false language=shell} echo $COGNITO_CLIENT_ID echo $CLIENT_SECRET echo $ISSUER :::\nVerification Copy output of echo https://$DATA_PLANE_LB/oidc-route/get and paste in browser.\nAfter accepting the Server Certificate, since you haven’t been authenticated, you will be redirected to Cognito’s Authentication page:\nClick on “Sign up” to register.\nAfter entering your data click on “Sign Up”. Cognito will create a user and request the verification code sent by your email.\nAfter typing the code, Cognito will authenticate you, issues an Authorization Code and redirects you back to the original URL (Data Plane). The Data Plane connects to Cognito with the Authorization Code to get the Access Token and then allows you to consume the URL.\nKong-gratulations! have now reached the end of this module by authenticating your API requests with AWS Cognito. You can now click Next to proceed with the next module.",
    "description": "OpenID Connect plugin allows the integration with a 3rd party identity provider (IdP) in a standardized way. This plugin can be used to implement Kong as a (proxying) OAuth 2.0 resource server (RS) and/or as an OpenID Connect relying party (RP) between the client, and the upstream service.\nIn this module, we will configure this plugin to use Amazon Cognito . A detailed integration guide is available here for future reading.\nCreating AWS Cognito Run the following command to create the AWS Cognito Resources using a CloudFormation templates",
    "tags": [],
    "title": "Authentication-OpenID Connect",
    "uri": "/12-api-gateway/15-use-cases/157-authenticaion_openid_connect/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-key-auth.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nbedrock services: name: service1 host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy instance_name: ai-proxy-bedrock-route enabled: true config: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: llm/v1/chat model: provider: bedrock options: bedrock: aws_region: us-west-2 name: key-auth instance_name: key-auth-bedrock enabled: true consumers: keyauth_credentials: key: “123456” username: user1 EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-key-auth.yaml :::\nVerify authentication is required New requests now require authentication\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is Jimi Hendrix?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nExpect response The response is a HTTP/1.1 401 Unauthorized, meaning the Kong Gateway Service requires authentication.\nHTTP/1.1 401 Unauthorized Date: Wed, 14 May 2025 18:38:01 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive WWW-Authenticate: Key Content-Length: 96 X-Kong-Response-Latency: 1 Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 8feb9f43ffb49565779f5a329cd33140 { \"message\":\"No API key found in request\", \"request_id\":\"8feb9f43ffb49565779f5a329cd33140\" } Send another request with an API key Use the apikey to pass authentication to access the services.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –header ‘apikey: 123456’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is Jimi Hendrix?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nThe request should now respond with a HTTP/1.1 200 OK.\nWhen submitting requests, the API Key name is defined, by default, apikey. You can change the plugin configuration, if you will.",
    "description": "In this section, you will configure the Key-Auth plugin on the Kong Route to protect Amazon Bedrock.\nAdd Kong Key Authentication plugin and Kong Consumer Add a KongPlugin resource for authentication, specifically the Key-Auth plugin. Note that, besides describing the plugin configuration, the declaration also creates a Kong Consumer, named user1, with an API Key (123456) as its credential.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-key-auth.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:",
    "tags": [],
    "title": "Key Auth Plugin",
    "uri": "/16-ai-gateway/17-use-cases/157-apikey/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:\nconsumer1: apikey = 123456 rate limiting policy = 500 tokens per minute consumer2: apikey = 987654 rate limiting policy = 10000 tokens per minute Doing that, the Data Plane is capable to not just protect the Route but to identify the consumer based on the key injected to enforce specific policies to the consumer. Keep in mind that a Consumer might have other plugins also enabled such as TCP Log, etc.\nNew Consumer and AI Rate Limiting Advanced plugin Policies Then, create the second consumer2, just like you did with the first one, with the 987654 key. Both Kong Consumers have the AI Rate Limiting Advanced plugin enabled with specific configurations.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-key-auth-rate-limiting-advanced.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nbedrock services: name: service1 host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy instance_name: ai-proxy-bedrock-route enabled: true config: auth: param_name: “allow_override” param_value: “false” param_location: “body” route_type: llm/v1/chat model: provider: bedrock options: bedrock: aws_region: us-west-2 name: key-auth instance_name: key-auth-bedrock enabled: true consumers: keyauth_credentials: key: “123456” username: user1 plugins: name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer1 config: llm_providers: name: bedrock window_size: 60 limit: 500 keyauth_credentials: key: “987654” username: user2\nplugins: name: ai-rate-limiting-advanced instance_name: ai-rate-limiting-advanced-consumer2 config: llm_providers: name: bedrock window_size: 60 limit: 10000 EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-key-auth-rate-limiting-advanced.yaml :::\nUse both Kong Consumers If you will, you can inject both keys to your requests.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –header ‘apikey: 123456’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is Jimi Hendrix?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nExpected output HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-bedrock: 500 X-AI-RateLimit-Remaining-minute-bedrock: 500 Date: Fri, 18 Apr 2025 14:58:51 GMT Content-Length: 2697 x-amzn-RequestId: 52a2aa58-fe6e-413b-a06c-889922bfdba7 X-Kong-LLM-Model: bedrock/us.amazon.nova-lite-v1:0 X-Kong-Upstream-Latency: 3155 X-Kong-Proxy-Latency: 4 Via: 1.1 kong/3.10.0.1-enterprise-edition Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 9782b97c3b0bda090f928d765214077b {\"usage\":{\"completion_tokens\":570,\"prompt_tokens\":5,\"total_tokens\":575},\"choices\":[{\"finish_reason\":\"stop\",\"message\":{\"content\":\"Jimi Hendrix was an American rock guitarist, singer, and songwriter widely regarded as one of the greatest and most influential guitarists in the history of popular music. Born on November 27, 1942, in Seattle, Washington, he became a pivotal figure in the 1960s psychedelic and hard rock movements.\\n\\n### Key Aspects of Jimi Hendrix's Career:\\n\\n1. **Early Life**:\\n - Hendrix grew up in Seattle and began playing guitar at a young age.\\n - He served in the U.S. Army but was discharged due to a foot injury.\\n\\n2. **Musical Breakthrough**:\\n - After moving to New York City, Hendrix became part of the vibrant music scene and played with various R\u0026B and soul bands.\\n - He moved to London in 1966, where he formed the Jimi Hendrix Experience with bassist Noel Redding and drummer Mitch Mitchell.\\n\\n3. **Iconic Albums**:\\n - **\\\"Are You Experienced\\\" (1967)**: His debut album, which included hits like \\\"Purple Haze\\\" and \\\"Hey Joe.\\\"\\n - **\\\"Axis: Bold as Love\\\" (1967)**: Showcased his innovative guitar work and songwriting.\\n - **\\\"Electric Ladyland\\\" (1968)**: Known for its ambitious and experimental tracks, including \\\"Voodoo Child (Slight Return)\\\" and \\\"All Along the Watchtower.\\\"\\n\\n4. **Innovative Techniques**:\\n - Hendrix revolutionized guitar playing with his use of feedback, distortion, and effects pedals.\\n - He was known for his ability to play the guitar with his teeth and behind his back, as well as his use of the wah-wah pedal.\\n\\n5. **Live Performances**:\\n - Hendrix's live performances were legendary, characterized by his energetic stage presence and improvisational skills.\\n - Notable performances include his iconic rendition of \\\"The Star-Spangled Banner\\\" at the Woodstock Festival in 1969 and his groundbreaking performance at the Monterey Pop Festival in 1967.\\n\\n6. **Legacy**:\\n - Hendrix's influence extends beyond rock music, impacting various genres including blues, funk, and heavy metal.\\n - He has been inducted into multiple halls of fame, including the Rock and Roll Hall of Fame and the UK Music Hall of Fame.\\n - Rolling Stone magazine has consistently ranked him among the greatest guitarists of all time.\\n\\n### Death:\\nHendrix died on September 18, 1970, in London, from asphyxiation due to barbiturate overdose. His death occurred just a day after his final concert at the Royal Albert Hall.\\n\\nJimi Hendrix remains a cultural icon, celebrated for his musical genius and his profound impact on the evolution of rock music.\",\"role\":\"assistant\"},\"index\":0}],\"object\":\"chat.completion\"} or\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –header ‘apikey: 987654’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is Jimi Hendrix?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nExpected output HTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-bedrock: 10000 X-AI-RateLimit-Remaining-minute-bedrock: 10000 Date: Fri, 18 Apr 2025 14:59:27 GMT Content-Length: 1670 x-amzn-RequestId: 814f0ce7-a47d-4e70-a6ce-be36e8e1b0b9 X-Kong-LLM-Model: bedrock/us.amazon.nova-lite-v1:0 X-Kong-Upstream-Latency: 2222 X-Kong-Proxy-Latency: 4 Via: 1.1 kong/3.10.0.1-enterprise-edition Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: a3a50214d6b82a6343f6e5608e56e633 {\"usage\":{\"completion_tokens\":330,\"prompt_tokens\":5,\"total_tokens\":335},\"choices\":[{\"finish_reason\":\"stop\",\"message\":{\"content\":\"Jimi Hendrix was an American rock guitarist, singer, and songwriter widely regarded as one of the greatest and most influential guitarists in the history of popular music. Born on November 27, 1942, in Seattle, Washington, he was a key figure in the evolution of rock music during the 1960s.\\n\\nHendrix gained fame in the mid-1960s while living in London, where he formed the Jimi Hendrix Experience with bassist Noel Redding and drummer Mitch Mitchell. His groundbreaking technique, which included the innovative use of feedback, distortion, and other effects, along with his virtuosic playing style, set him apart from his contemporaries.\\n\\nSome of Hendrix's most iconic songs include \\\"Purple Haze,\\\" \\\"Foxy Lady,\\\" \\\"Hey Joe,\\\" \\\"All Along the Watchtower,\\\" and \\\"Voodoo Child (Slight Return).\\\" His performances, particularly at the Monterey Pop Festival in 1967, Woodstock in 1969, and the Isle of Wight Festival in 1970, are legendary.\\n\\nHendrix's influence extends far beyond his recorded music. He was known for his charismatic stage presence and his ability to push the boundaries of what was possible with the electric guitar. His work has inspired countless musicians across various genres.\\n\\nTragically, Hendrix's career was cut short when he died on September 18, 1970, in London, from asphyxiation due to barbiturate overdose. Despite his relatively short career, his impact on music and culture endures, and he continues to be celebrated as a pioneering artist.\",\"role\":\"assistant\"},\"index\":0}],\"object\":\"chat.completion\"} Again, test the rate-limiting policy by executing the following command multiple times and observe the rate-limit headers in the response, specially, X-AI-RateLimit-Limit-minute-bedrock and X-AI-RateLimit-Remaining-minute-bedrock:\nNow, let’s consume it with the Consumer1’s API Key. As you can see the Data Plane is processing the Rate Limiting processes independently.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –header ‘apikey: 123456’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is Jimi Hendrix?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nHTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-bedrock: 500 X-AI-RateLimit-Remaining-minute-bedrock: 500 Date: Fri, 18 Apr 2025 15:00:53 GMT Content-Length: 2601 x-amzn-RequestId: 998b5022-e5b2-487f-a3d0-cb19eb8dee77 X-Kong-LLM-Model: bedrock/us.amazon.nova-lite-v1:0 X-Kong-Upstream-Latency: 3287 X-Kong-Proxy-Latency: 4 Via: 1.1 kong/3.10.0.1-enterprise-edition Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: ba97994f991645566325aa5468568e72 If we keep sending requests using the first API Key, eventually, as expected, we’ll get an error code:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –header ‘apikey: 123456’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is Jimi Hendrix?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nHTTP/1.1 429 Too Many Requests Date: Fri, 18 Apr 2025 15:09:57 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-AI-RateLimit-Reset: 8 X-AI-RateLimit-Retry-After-minute-bedrock: 8 X-AI-RateLimit-Retry-After: 8 X-AI-RateLimit-Limit-minute-bedrock: 500 X-AI-RateLimit-Remaining-minute-bedrock: 0 X-AI-RateLimit-Reset-minute-bedrock: 8 Content-Length: 67 X-Kong-Response-Latency: 1 Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 671b855c4535e7596d22d16cca0e6f10 {\"message\":\"AI token rate limit exceeded for provider(s): bedrock\"} However, the second API Key is still allowed to consume the Kong Route:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -i -X POST –url $DATA_PLANE_LB/bedrock-route –header ‘Content-Type: application/json’ –header ‘apikey: 987654’ –data ‘{ “messages”: [ { “role”: “user”, “content”: “Who is Jimi Hendrix?” } ], “model”: “us.amazon.nova-lite-v1:0” }’ :::\nHTTP/1.1 200 OK Content-Type: application/json Connection: keep-alive X-AI-RateLimit-Limit-minute-bedrock: 10000 X-AI-RateLimit-Remaining-minute-bedrock: 10000 Date: Fri, 18 Apr 2025 15:10:30 GMT Content-Length: 2287 x-amzn-RequestId: a5ffda3e-0ed0-4958-9134-382dc4cc87e7 X-Kong-LLM-Model: bedrock/us.amazon.nova-lite-v1:0 X-Kong-Upstream-Latency: 2974 X-Kong-Proxy-Latency: 2 Via: 1.1 kong/3.10.0.1-enterprise-edition Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 55c8f21ff35a8043d33ed739ec4aac1c {\"usage\":{\"completion_tokens\":480,\"prompt_tokens\":5,\"total_tokens\":485},\"choices\":[{\"finish_reason\":\"stop\",\"message\":{\"content\":\"Jimi Hendrix was an American rock guitarist, singer, and songwriter widely regarded as one of the greatest and most influential guitarists in the history of popular music. Born on November 27, 1942, in Seattle, Washington, he became a pivotal figure in the 1960s psychedelic and hard rock movements.\\n\\n### Early Life and Career\\n- **Birth Name:** Johnny Allen Hendrix\\n- **Early Years:** He grew up in Seattle and began playing guitar at a young age. His early career included stints in various R\u0026B bands in the Southern United States.\\n\\n### Breakthrough\\n- **Move to New York:** Hendrix moved to New York City in 1964 and later to London in 1966, where he found greater success.\\n- **The Jimi Hendrix Experience:** In London, he formed the Jimi Hendrix Experience with bassist Noel Redding and drummer Mitch Mitchell.\\n\\n### Musical Innovations\\n- **Guitar Techniques:** Hendrix was known for his innovative use of guitar effects, including distortion, feedback, and wah-wah pedals. He also played the guitar behind his back and with his teeth.\\n- **Stage Presence:** His electrifying performances and flamboyant stage presence made him a legend.\\n\\n### Notable Works\\n- **Albums:** Some of his most famous albums include \\\"Are You Experienced\\\" (1967), \\\"Axis: Bold as Love\\\" (1967), and \\\"Electric Ladyland\\\" (1968).\\n- **Songs:** Iconic songs include \\\"Purple Haze,\\\" \\\"Hey Joe,\\\" \\\"All Along the Watchtower,\\\" and \\\"Voodoo Child (Slight Return).\\\"\\n\\n### Legacy\\n- **Influence:** Hendrix's influence extends across genres, impacting rock, blues, funk, and jazz musicians.\\n- **Death:** Tragically, he died on September 18, 1970, in London, at the age of 27, from an overdose of barbiturates.\\n\\n### Honors and Recognition\\n- **Hall of Fame:** Inducted into the Rock and Roll Hall of Fame in 1990.\\n- **Cultural Impact:** His music and persona continue to be celebrated and studied, and he remains a symbol of artistic freedom and innovation.\\n\\nJimi Hendrix's contributions to music have left an indelible mark, and his work continues to inspire new generations of musicians and fans alike.\",\"role\":\"assistant\"},\"index\":0}],\"object\":\"chat.completion\"} Kong-gratulations! have now reached the end of this module by authenticating the API requests with a key and associating different consumers with policy plans. You can now click Next to proceed with the next module.",
    "description": "With the existing API Key policy, we can control the incoming requests. However, the policies implemented by the other plugins are the same regardless the consumer.\nIn this section, we are going to define specific Rate Limiting policies for each Consumer represented by its API Key.\nKong Consumer Policies It’s important then to be able to define specific policies for each one of these consumers. For example, it would be great to define Rate Limiting policies for different consumers like this:",
    "tags": [],
    "title": "AI Rate Limiting Advanced plugin",
    "uri": "/16-ai-gateway/17-use-cases/158-rate-limiting/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:\nLowest-usage Round-robin (weighted) Consistent-hashing (sticky-session on given header value) Semantic routing The AI Proxy Advanced plugin supports semantic routing, which enables distribution of requests based on the similarity between the prompt and the description of each model. This allows Kong to automatically select the model that is best suited for the given domain or use case.\nBy analyzing the content of the request, the plugin can match it to the most appropriate model that is known to perform better in similar contexts. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.\nAs a illustration here is the architecture where we are going to implement the multiple load balancing policies. AI Proxy Advanced will manage both Bedrock’s LLM models:\nus.amazon.nova-lite-v1:0 us.meta.llama3-3-70b-instruct-v1:0 You can now click Next to proceed further.",
    "description": "The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.\nThe plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.\nLoad balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:",
    "tags": [],
    "title": "AI Proxy Advanced plugin",
    "uri": "/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway",
    "content": "Kong AI Gateway With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.\nYou can enable the AI Gateway features through a set of specialized plugins, using the same model you use for any other Kong Gateway plugin.\nKong AI Gateway functional scope Amazon Bedrock Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.\nUsing Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don’t have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.\nThe use case describe in this workshop will consume some Foundation Models, including:\nAmazon: Nova Lite - amazon.nova-lite-v1:0 Nova Micro - amazon.nova-micro-v1:0 Titan Text Embeddings V2 - amazon.titan-embed-text-v2:0 Meta: Llama 3.3 70B Instruct v1 - meta.llama3-3-70b-instruct-v1:0 High Level Tasks You will complete the following:\nSet up Kong AI Proxy for Bedrock Integration Implement Kong AI Plugins to secure prompt message You can now click Next to proceed further.",
    "description": "Kong AI Gateway With the rapid emergence of multiple AI LLM providers, the AI technology landscape is fragmented and lacking in standards and controls. Kong AI Gateway is a powerful set of features built on top of Kong Gateway, designed to help developers and organizations effectively adopt AI capabilities quickly and securely\nWhile AI providers don’t conform to a standard API specification, the Kong AI Gateway provides a normalized API layer allowing clients to consume multiple AI services from the same client code base. The AI Gateway provides additional capabilities for credential management, AI usage observability, governance, and tuning through prompt engineering. Developers can use no-code AI Plugins to enrich existing API traffic, easily enhancing their existing application functionality.",
    "tags": [],
    "title": "AI with Amazon Bedrock",
    "uri": "/16-ai-gateway/159-ai-gateway-bedrock/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "In this second part of the workshop we are going to explore the AI capabilities provides by Kong AI Gateway and the specific collection of plugins.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Kong AI Gateway",
    "description": "In this second part of the workshop we are going to explore the AI capabilities provides by Kong AI Gateway and the specific collection of plugins.\nYou can now click Next to begin the module.\nOptional Reading Learn more about Kong AI Gateway",
    "tags": [],
    "title": "Kong AI Gateway",
    "uri": "/16-ai-gateway/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway \u003e Use Cases",
    "content": "What is RAG? RAG offers a solution to some of the limitations in traditional generative AI models, such as accuracy and hallucinations, allowing companies to create more contextually relevant AI applications.\nBasically, the RAG application comprises two main processes:\nData Preparation: External data sources, including all sorts of documents, are chunked and submitted to an Embedding Model which converts them into embeddings. The embeddings are stored in a Vector Database.\nQuery time: A consumer wants to send a query to the actual Prompt/Chat LLM model. However, the query should be enhanced with relevant data, taken from the Vector Database, and not available in the LLM model. The following steps are then performed:\nThe consumer builds a Prompt. The RAG application converts the Prompt into an embedding, calling the Embedding Model. Leveraging Semantic Search, RAG matches the Prompt Embedding with the most relevant information and retrieves that Vector Database. The Vector Database returns relevant data as a response to the search query. The RAG application sends a query to the Prompt/Chat LLM Model combining the Prompt with the Relevant Data returned by the Vector Database. The LLM Model returns a response. Implementation Architecture Data Preparation time During the preparation time, the following steps are executed:\nThe Document Loader script asks the AI RAG Injector plugin to provides the configuration regarding both Embedding Model and Vector Database. The Document Loader sends data chunks to the Embedding Model to gets the embeddings related to them The Document Loader stores the embeddings and content into the Vector Database. RAG time The following steps are performed during the execution time:\nThe API Consumer send a request with a prompt. The AI RAG Injector Plugin converts the prompt into embeddings calling the Embedding Model. The AI RAG Injector Plugin sends a KNN vector search query to the Vector Database to find the top “k-nearest neighbors” to a query vector. The AI Proxy Advanced Plugin sends a regular request to the LLM model adding the relevant data received from the Vector Database. Here’s the declaration including both plugins: AI RAG Injector and AI Proxy Advanced.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e ai-rag-injector.yaml « ‘EOF’ _format_version: ‘3.0’ _konnect: control_plane_name: kong-aws services:\nname: ai-proxy url: http://localhost:65535 routes: name: bedrock-route paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: targets: logging: log_statistics: true route_type: llm/v1/chat auth: param_name: “allow_override” param_value: “false” param_location: “body” model: name: “us.amazon.nova-micro-v1:0” provider: bedrock options: bedrock: aws_region: “us-west-2” name: ai-rag-injector id: 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1 instance_name: ai-rag-injector-bedrock config: inject_template: | Only use the following information surrounded by to and your existing knowledge to provide the best possible answer to the user. User’s question: fetch_chunks_count: 1 embeddings: auth: param_name: “allow_override” param_value: “false” param_location: “body” model: provider: bedrock name: “amazon.titan-embed-text-v2:0” options: bedrock: aws_region: “us-west-2” vectordb: strategy: redis redis: host: redis-stack.redis port: 6379 distance_metric: cosine #dimensions: 768 dimensions: 1024 EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-rag-injector.yaml :::\nIf you send a request with no context, we’ll see some sort of hallucination: :::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header “Content-Type: application/json” –data ‘{ “messages”: [ { “role”: “user”, “content”: “What did Marco say about AI Gateways?” } ] }’ | jq ‘.choices[].message.content’ :::\nTypical response: \"It seems like there might be a bit of confusion here. Marco Polo was a Venetian explorer who traveled through Asia in the 13th century, and he is best known for his book \\\"The Travels of Marco Polo,\\\" which detailed his journeys and experiences in the East. Given the historical context, Marco Polo wouldn't have anything to say about AI Gateways, as this concept is very modern and pertains to artificial intelligence and data management systems.\\n\\nIf you're referring to a different Marco who has commented on AI Gateways, it would be helpful to have more context or specify which Marco you are talking about. AI Gateways typically refer to systems that manage and facilitate the interaction between different AI applications, data sources, and services, often focusing on data integration, security, and interoperability.\\n\\nIf you provide more details, I can offer a more precise and relevant response.\" We are going to inject some context using an interview transcript snippet Marco Palladino, Kong’s co-founder and CTO, gave some time ago.\nThe transcript snippet is available in the following file.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl ‘:assetUrl{path=\"/code/SED1683-Kong-short.txt\"}’ –output ./SED1683-Kong-short.txt :::\nWe have to copy the Document Loader script, algo available in a file, to the Data Plane: :::code{showCopyAction=true showLineNumbers=false language=shell} curl ‘:assetUrl{path=\"/code/ingest_update.lua “}’ –output ./ingest_update.lua kubectl cp ./ingest_update.lua -n kong $(kubectl get pod -n kong -o json | jq -r ‘.items[].metadata | select(.name | startswith(“dataplane-”))’ | jq -r ‘.name’):/tmp/ingest_update.lua :::\nNow, execute the Document Loader passing the content as a parameter. Note that, to make to process a bit easier, we have created the AI RAG Injector plugin with a pre-defined id.\n:::code{showCopyAction=true showLineNumbers=false language=shell} kubectl exec -ti $(kubectl get pod -n kong -o json | jq -r ‘.items[].metadata | select(.name | startswith(“dataplane-”))’ | jq -r ‘.name’) -n kong – kong runner /tmp/ingest_update.lua 3194f12e-60c9-4cb6-9cbc-c8fd7a00cff1 “’\"$(cat ./SED1683-Kong-short.txt)”’” :::\nYou should be able to get a much better response from the LLM model this time:\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -s -X POST –url $DATA_PLANE_LB/bedrock-route –header “Content-Type: application/json” –data ‘{ “messages”: [ { “role”: “user”, “content”: “What did Marco say about AI Gateways?” } ] }’ | jq ‘.choices[].message.content’ :::\n\"Based on the provided information, Marco mentioned that Kong provides a unified control plane that allows for the deployment of different gateways, including an AI gateway. Specifically, he explained that the AI gateway is used for connecting together multiple large language models (LLMs) and orchestrating them. This suggests that Kong's platform offers a standardized way to manage, consume, monitor, and expose APIs across various use cases, including those involving AI and machine learning technologies.\"",
    "description": "What is RAG? RAG offers a solution to some of the limitations in traditional generative AI models, such as accuracy and hallucinations, allowing companies to create more contextually relevant AI applications.\nBasically, the RAG application comprises two main processes:\nData Preparation: External data sources, including all sorts of documents, are chunked and submitted to an Embedding Model which converts them into embeddings. The embeddings are stored in a Vector Database.",
    "tags": [],
    "title": "RAG - Retrieval-Augmented Generation",
    "uri": "/16-ai-gateway/17-use-cases/170-rag/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong AI Gateway",
    "content": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformer AI Semantic Cache AI Rate Limiting AI Proxy Advanced and load balancing algoritms RAG These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.\nYou can now click Next to begin the module.",
    "description": "In this chapter we are going to explore the following common use cases we typically implement at the API Gateway Layer.\nSimple AI Gateway Proxy Prompt Engineering AI Request and Response Tranformer AI Semantic Cache AI Rate Limiting AI Proxy Advanced and load balancing algoritms RAG These functionalities are extended by the use of Kong Plugins. You can find a full list of all Kong AI plugins on the Plugin Hub.",
    "tags": [],
    "title": "Use Cases",
    "uri": "/16-ai-gateway/17-use-cases/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e Rate Limiting Using Redis",
    "content": "Add Rate Limiting plugin Just like you did before, add the Rate Limiting plugin on the Route setting Minute as 5 requests per minute, and set the identifier to Service.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e rate-limiting.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: rate-limiting-route paths: /rate-limiting-route plugins: name: rate-limiting instance_name: rate-limiting1 config: minute: 5 EOF ::: Submit the declaration:\n:::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway sync –konnect-token $PAT rate-limiting.yaml :::\nVerify traffic control Again, test the rate-limiting policy by executing the following command multiple times and observe the rate-limit headers in the response, specially, X-RateLimit-Remaining-Minute, RateLimit-Reset and Retry-After :\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -I $DATA_PLANE_LB/rate-limiting-route/get :::\nResponse\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 455 Connection: keep-alive RateLimit-Limit: 5 RateLimit-Reset: 10 RateLimit-Remaining: 4 X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 4 Server: gunicorn Date: Wed, 28 May 2025 12:30:50 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 1 X-Kong-Proxy-Latency: 1 Via: 1.1 kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 6209954ef700ea7b7b23b8003b318e74 As explected, after sending too many requests,once the rate limiting is reached, you will see HTTP/1.1 429 Too Many Requests\n# curl -I $DATA_PLANE_LB/rate-limiting-route/get HTTP/1.1 429 Too Many Requests Date: Wed, 28 May 2025 12:30:55 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive RateLimit-Limit: 5 Retry-After: 5 RateLimit-Reset: 5 RateLimit-Remaining: 0 X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 0 Content-Length: 92 X-Kong-Response-Latency: 0 Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 597fcd055d14c0189a6224041553e267 Results As there is a single Kong Data Plane Runtime instance running, Kong correctly imposes the rate-limit and you can make only 5 requests in a minute.",
    "description": "Add Rate Limiting plugin Just like you did before, add the Rate Limiting plugin on the Route setting Minute as 5 requests per minute, and set the identifier to Service.\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e rate-limiting.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: rate-limiting-route paths: /rate-limiting-route plugins: name: rate-limiting instance_name: rate-limiting1 config: minute: 5 EOF ::: Submit the declaration:",
    "tags": [],
    "title": "Set up Rate Limiting plugin",
    "uri": "/12-api-gateway/15-use-cases/156-rate-limiting-using-redis/1563-setup-rate-limiting-plugin/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e Rate Limiting Using Redis",
    "content": "Let’s scale out the Kong Data Plane deployment to 3 pods, for scalability and redundancy:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat «EOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions:\nkind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.10.0.1 serviceAccountName: kaigateway-podid-sa replicas: 3 network: services: ingress: name: proxy1 type: LoadBalancer annotations: “service.beta.kubernetes.io/aws-load-balancer-scheme”: “internet-facing” “service.beta.kubernetes.io/aws-load-balancer-nlb-target-type”: “ip” EOF ::: Wait for replicas to deploy It will take a couple minutes for the new pods to start up. Run the following command to show that the replicas are ready.\n:::code{showCopyAction=true showLineNumbers=false language=shell} kubectl get pods -n kong :::\nNAME READY STATUS RESTARTS AGE dataplane-dataplane1-qdc66-84d7746bbf-dnvp8 1/1 Running 0 4d23h dataplane-dataplane1-qdc66-84d7746bbf-hlxwx 1/1 Running 0 26s dataplane-dataplane1-qdc66-84d7746bbf-kpbpl 1/1 Running 0 26s httpbin-5c69574c95-xq76q 1/1 Running 0 6d19h Check Konnect Runtime Group Similarly you can see new Runtime Instances connected to your Runtime Group\nVerify traffic control Test the rate-limiting policy by executing the following command and observing the rate-limit headers.\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -I $DATA_PLANE_LB/rate-limiting-route/get :::\nResponse\nHTTP/1.1 200 OK Content-Type: application/json Content-Length: 455 Connection: keep-alive RateLimit-Reset: 29 X-RateLimit-Remaining-Minute: 4 X-RateLimit-Limit-Minute: 5 RateLimit-Limit: 5 RateLimit-Remaining: 4 Server: gunicorn Date: Wed, 28 May 2025 12:35:31 GMT Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true X-Kong-Upstream-Latency: 1 X-Kong-Proxy-Latency: 6 Via: 1.1 kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: 2373ed35e4fff87e406eb4347e15702f Results You will observe that the rate-limit is not consistent anymore and you can make more than 5 requests in a minute.\nTo understand this behavior, we need to understand how we have configured Kong. In the current policy, each Kong node is tracking a rate-limit in-memory and it will allow 5 requests to go through for a client. There is no synchronization of the rate-limit information across Kong nodes. In use-cases where rate-limiting is used as a protection mechanism and to avoid over-loading your services, each Kong node tracking it’s own counter for requests is good enough as a malicious user will hit rate-limits on all nodes eventually. Or if the load-balance in-front of Kong is performing some sort of deterministic hashing of requests such that the same Kong node always receives the requests from a client, then we won’t have this problem at all.\nWhats Next ? In some cases, a synchronization of information that each Kong node maintains in-memory is needed. For that purpose, Redis can be used. Let’s go ahead and set this up next.",
    "description": "Let’s scale out the Kong Data Plane deployment to 3 pods, for scalability and redundancy:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat «EOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions:\nkind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.10.0.1 serviceAccountName: kaigateway-podid-sa replicas: 3 network: services: ingress: name: proxy1 type: LoadBalancer annotations: “service.beta.kubernetes.io/aws-load-balancer-scheme”: “internet-facing” “service.beta.kubernetes.io/aws-load-balancer-nlb-target-type”: “ip” EOF ::: Wait for replicas to deploy It will take a couple minutes for the new pods to start up. Run the following command to show that the replicas are ready.",
    "tags": [],
    "title": "Scale Kong for Kubernetes to multiple pods",
    "uri": "/12-api-gateway/15-use-cases/156-rate-limiting-using-redis/1564-scale-kong-for-kubernetes-to-multiple-pods/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect \u003e Kong API Gateway \u003e Use Cases \u003e Rate Limiting Using Redis",
    "content": "Update the Rate Limiting Plugin Let’s update our Kong Plugin configuration to use Redis as a data store rather than each Kong node storing the counter information in-memory. As a reminder, Redis was installed previously and it is available in the EKS cluster.\nHere’s the new declarion:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e rate-limiting.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: rate-limiting-route paths: /rate-limiting-route plugins: name: rate-limiting instance_name: rate-limiting1 config: minute: 5 policy: redis redis: host: redis-stack.redis.svc.cluster.local port: 6379 EOF ::: Submit the declaration:\n:::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway sync –konnect-token $PAT rate-limiting.yaml :::\nTest it Execute the following commands more than 5 times.\nWhat happens?\n:::code{showCopyAction=true showLineNumbers=false language=shell} curl -I $DATA_PLANE_LB/rate-limiting-route/get :::\nResponse\nHTTP/1.1 429 Too Many Requests Date: Wed, 28 May 2025 12:40:06 GMT Content-Type: application/json; charset=utf-8 Connection: keep-alive X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 0 RateLimit-Reset: 54 Retry-After: 54 RateLimit-Remaining: 0 RateLimit-Limit: 5 Content-Length: 92 X-Kong-Response-Latency: 2 Server: kong/3.10.0.1-enterprise-edition X-Kong-Request-Id: d517af1d0873f14cca18af72d314072c Expected Results Because Redis is the data-store for the rate-limiting plugin, you should be able to make only 5 requests in a minute\nReduce the number of replicas :::code{showCopyAction=true showLineNumbers=false language=shell} cat «EOF | kubectl apply -f - apiVersion: gateway-operator.konghq.com/v1beta1 kind: DataPlane metadata: name: dataplane1 namespace: kong spec: extensions:\nkind: KonnectExtension name: konnect-config1 group: konnect.konghq.com deployment: podTemplateSpec: spec: containers: - name: proxy image: kong/kong-gateway:3.10.0.1 serviceAccountName: kaigateway-podid-sa replicas: 1 network: services: ingress: name: proxy1 type: LoadBalancer annotations: “service.beta.kubernetes.io/aws-load-balancer-scheme”: “internet-facing” “service.beta.kubernetes.io/aws-load-balancer-nlb-target-type”: “ip” EOF ::: Kong-gratulations! have now reached the end of this section by configuring Redis as a data-store to synchronize information across multiple Kong nodes to enforce the rate-limiting policy. This can also be used for other plugins which support Redis as a data-store such as proxy-cache. You can now click Next to proceed with the next section of the module.",
    "description": "Update the Rate Limiting Plugin Let’s update our Kong Plugin configuration to use Redis as a data store rather than each Kong node storing the counter information in-memory. As a reminder, Redis was installed previously and it is available in the EKS cluster.\nHere’s the new declarion:\n:::code{showCopyAction=true showLineNumbers=false language=shell} cat \u003e rate-limiting.yaml « ‘EOF’ _format_version: “3.0” _konnect: control_plane_name: kong-aws _info: select_tags:\nhttpbin-service-route services: name: httpbin-service host: httpbin.kong.svc.cluster.local port: 8000 routes: name: rate-limiting-route paths: /rate-limiting-route plugins: name: rate-limiting instance_name: rate-limiting1 config: minute: 5 policy: redis redis: host: redis-stack.redis.svc.cluster.local port: 6379 EOF ::: Submit the declaration:",
    "tags": [],
    "title": "Update the Rate Limiting Plugin",
    "uri": "/12-api-gateway/15-use-cases/156-rate-limiting-using-redis/1565-update-rate-limiting-plugin/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong, while the runtime environments are deployed in your AWS accounts. Management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.\nLearning Objectives In this workshop, you will:\nGet an architectural overview of Kong Konnect platform. Set up Konnect runtime on Amazon Elastic Kubernetes Service (EKS). Learn what are services, routes and plugin. Deploy a sample microservice and access the application using the defined route. Use the platform to address the following API Gateway use cases Authentication and Authorization Rate limiting Response Transformer Proxy caching And the following AI Gateway use cases Prompt Engineering LLM-based Request and Reponse transformation Semantic Caching Token-based Rate Limiting Semantic Routing RAG - Retrieval-Augmented Generation Expected Duration Pre-Requisite Environment Setup (20 minutes) Architectural Walkthrough (10 minutes) Sample Application and addressing the use cases (60 minutes) Next Steps and Cleanup (5 min) :::alert{header=“Important” type=“warning”}\nRunning this workshop in your own AWS account will incur ~ $1 per hour charges. Please ensure to cleanup, once you fulfill your learning objectives.",
    "description": "Introduction Kong Konnect is an API lifecycle management platform delivered as a service. The management plane is hosted in the cloud by Kong, while the runtime environments are deployed in your AWS accounts. Management plane enables customers to securely execute API management activities such as create API routes, define services etc. Runtime environments connect with the management plane using mutual transport layer authentication (mTLS), receive the updates and take customer facing API traffic.",
    "tags": [],
    "title": "API Management with Kong Konnect",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "API Management with Kong Konnect",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
