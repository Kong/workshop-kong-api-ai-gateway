<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Proxy Advanced plugin :: API Management with Kong Konnect</title>
    <link>http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/index.html</link>
    <description>The AI Proxy Advanced plugin lets you transform and proxy requests to multiple AI providers and models at the same time. This lets you set up load balancing between targets.&#xA;The plugin accepts requests in one of a few defined and standardised formats, translates them to the configured target format, and then transforms the response back into a standard format.&#xA;Load balancing This plugin supports several load-balancing algorithms, similar to those used for Kong upstreams, allowing efficient distribution of requests across different AI models. The supported algorithms include:</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Round Robin</title>
      <link>http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/round-robin/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/round-robin/index.html</guid>
      <description>Let’s get started with a simple round-robin policy:&#xA;:::code{showCopyAction=true showLineNumbers=false language=shell} cat &gt; ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:&#xA;bedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: balancer: algorithm: round-robin targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy-advanced.yaml :::</description>
    </item>
    <item>
      <title>Weight</title>
      <link>http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/weight/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/weight/index.html</guid>
      <description>Now, let’s redirect 80% of the request to Amazon’s Nova with a weight based policy:&#xA;:::code{showCopyAction=true showLineNumbers=false language=shell} cat &gt; ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:&#xA;bedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false weight: 80 model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false weight: 20 EOF ::: Apply the declaration with decK: :::code{showCopyAction=true showLineNumbers=false language=shell} deck gateway reset –konnect-control-plane-name kong-aws –konnect-token $PAT -f deck gateway sync –konnect-token $PAT ai-proxy-advanced.yaml :::</description>
    </item>
    <item>
      <title>Lowest-Latency and Lowest-Usage</title>
      <link>http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/lowest-latency-usage/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/lowest-latency-usage/index.html</guid>
      <description>Lowest Latency policy The lowest-latency algorithm is based on the response time for each model. It distributes requests to models with the lowest response time.&#xA;Create a file with the following declaration:&#xA;:::code{showCopyAction=true showLineNumbers=false language=shell} cat &gt; ai-proxy-advanced.yaml « ‘EOF’ _format_version: “3.0” _info: select_tags:&#xA;bedrock _konnect: control_plane_name: kong-aws services: name: ai-proxy-advanced-service host: localhost port: 32000 routes: name: route1 paths: /bedrock-route plugins: name: ai-proxy-advanced instance_name: ai-proxy-advanced-bedrock config: balancer: algorithm: lowest-latency latency_strategy: e2e targets: model: provider: bedrock name: “us.amazon.nova-micro-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false model: provider: bedrock name: “us.meta.llama3-3-70b-instruct-v1:0” options: bedrock: aws_region: us-west-2 route_type: “llm/v1/chat” auth: allow_override: false EOF ::: Apply the declaration with decK:</description>
    </item>
    <item>
      <title>Semantic Routing</title>
      <link>http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/semantic-routing/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/16-ai-gateway/17-use-cases/159-ai-proxy-advanced/semantic-routing/index.html</guid>
      <description>Semantic The semantic algorithm distributes requests to different models based on the similarity between the prompt in the request and the description provided in the model configuration. This allows Kong to automatically select the model that is best suited for the given domain or use case. This feature enhances the flexibility and efficiency of model selection, especially when dealing with a diverse range of AI providers and models.</description>
    </item>
  </channel>
</rss>